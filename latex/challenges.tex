Before delving into our analysis, we highlight below several difficulties in evaluating the performance of fine-tuning algorithms. These complexities caution against simplistic comparisons and underscore the importance of a more comprehensive evaluation framework, as what we proposed in \cref{sec:evaluation}.



\textbf{Sensitivity to Hyperparameters.}
    As we can see from \cref{fig:plots-main}, the algorithms' performance is sensitive to various hyperparameters.
    %such as learning rate, dimension, and factor. 
    Without a single objective metric, pinpointing the optimal hyperparameters for a specific method becomes elusive. As a result, comparing two methods based on a single set of hyperparameters oversimplifies the evaluation process.

\textbf{Discrepancy of Results Across Concepts.}
    The models' performances can differ substantially across different concepts, whether these are trained in isolation or in tandem.
    This variance is especially pronounced when comparing concepts that are fundamentally different or with differing numbers of training images.
    Precisely because of this, we opt to analyze the results for different categories separately.
    Nonetheless, even when these factors are mitigated, discrepancies in performance across concepts can still arise, as illustrated in \cref{apx:class-difference}.

\textbf{Conflicting Criteria.}
    There's an intrinsic trade-off between the various criteria under consideration. For instance, models with higher concept fidelity often have lower controllability, diversity, and base model preservation. Determining the optimal balance among these criteria requires a nuanced, case-by-case analysis rather than a simple aggregation of metrics.
    
 \textbf{Unreliability of Evaluation Metrics.}
    Despite the significant advancements in computer vision and deep learning in recent years, the metrics we employ are not still far from perfect. Often, these metrics do not fully align with human perception and may overlook nuanced details that define a concept, as elaborated in \cref{apx:unreliability}. Additionally, a single numerical value can be insufficiently informative; for instance, a low image similarity score could arise from either underfitting or overfitting. To account for such shortcomings,
    we supplement our quantitative evaluations with visual inspections throughout our experiments for a more rounded evaluation. %This ensures a more comprehensive understanding of model performance and mitigates the limitations of solely relying on numerical metrics.


\textbf{Practical Considerations.}
    In practice, a trained model often undergoes further adjustments. 
    For example, we can adjust the scaling factor of the fine-tuned weights (an experiment dedicated to this is provided in \needref), combine multiple trained networks, and apply the fine-tuned parameters to a different base model. This is not to mention the influence of prompts and the potential for prompt engineering. All these variables add layers of complexity to the evaluations.