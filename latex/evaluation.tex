With the wide range of algorithmic choices and hyperparameter settings made possible by \lycoris, one naturally wonders: Is there an optimal algorithm or set of hyperparameters for fine-tuning Stable Diffusion? To tackle this question in a comprehensive manner, it is essential to first establish a clear framework for model evaluation.

With this in mind, in this section, we turn our focus to two independent but intertwined components that are crucial for a systematic evaluation of fine-tuned text-to-image models:
\begin{enumerate*}[\itshape i\upshape)]
    \item the types of prompts used for image generation and
    \item the evaluation of the generated images. 
\end{enumerate*}
While these two components are commonly considered as a single entity in existing literature, explicitly distinguishing between them allows for a more nuanced evaluation of model performance (see \cref{apx:related-works}
for a comprehensive overview of related works on text-to-image model evaluation). Below, we explore each of these components in detail.

%This distinction, although often implicit in existing work, is crucial for a systematic and comprehensive assessment of model performance. Below, we explore each of these components in detail.

\subsection{Classification of Prompts for Image Generation}
\label{subsec:prompt-classif}

To fully understand the model's behavior, it is important to distinguish between different types of prompts that guide image generation. We categorize these into three main types as follows:

\begin{itemize}
\item \textbf{Training Prompts}: These are the prompts originally used for training the model. The images generated from these prompts are expected to closely align with the training dataset, providing insight into how well the model has captured the target concepts.
\item \textbf{Generalization Prompts}:
These prompts seek to generate images that generalize learned concepts to broader contexts, going beyond the specific types of images encountered in the training set.
This includes, for example, combining the innate knowledge of the base model with the learned concepts,
combining concepts trained within the same model, and combining concepts trained across different models which are later merged together.
Such prompts are particularly useful to evaluate the disentanglement of the learned representations.
\item \textbf{Concept-Agnostic Prompts}: These are prompts that deliberately avoid using trigger words from the training set and are often employed to assess concept leak, see \eg \citet{kumari2023multi}. When training also involves class words, this category can be further refined to distinguish between prompts that do and do not use these class words.
\end{itemize}



\subsection{Evaluation Criteria}
\label{subsec:eval-criteria}

After detailing the different types of prompts that guide the image generation process, the next important step is to identify the aspects that we would like to look at when evaluating the generated images, as we outline below.

\begin{itemize}
\item \textbf{Fidelity} measures the extent to which generated images adhere to the target concept.
\item \textbf{Controllability} evaluates the model's ability to generate images that align well with text prompts.
\item \textbf{Diversity} assesses the variety of images that are produced from a single or a set of prompts.
\item \textbf{Base Model Preservation} measures how much fine-tuning affects the base model's inherent capabilities, particularly in ways that may be undesirable. For example, if the target concept is an object, retaining the background and style as generated by the base model might be desired.
\item \textbf{Image Quality} concerns the visual appeal of the generated images, focusing primarily on aspects like naturalness, absence of artifacts, and lack of weird deformations. Aesthetics, though related, are considered to be more dependent on the dataset than on the training method, and are therefore not relevant for our purpose.
\end{itemize}

Taken together, the prompt classification of \cref{subsec:prompt-classif} and the evaluation criteria listed above offer a nuanced and comprehensive framework for assessing fine-tuned text-to-image models.
Notably, these tools also enable us to evaluate other facets of model performance, such as the ability to learn multiple distinct concepts without mutual interference and the capability for parallel training of multiple models that can later be successfully merged.