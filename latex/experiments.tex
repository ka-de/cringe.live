In this section, we perform extensive experiments to compare different \lycoris~algorithms and to assess the impact of the hyperparameters. Our experiments employ the non-EMA version of Stable Diffusion 1.5
%\footnote{\url{https://huggingface.co/runwayml/stable-diffusion-v1-5}}
as the base model.
%An additional set of experiments that uses an anime-oriented based model are presented in \needref.
All the experimental details not included in the main text along with presentations of additional experiments can be found in the appendix.
%, including one that uses an anime-oriented based model can be found in the appendix.

\subsection{Dataset}

Contrary to prior studies that primarily focus on single-concept fine-tuning with very few images, we consider a dataset that spans across a wide variety of concepts with an imbalance in the number of images for each.
%This corresponds to a moderate scale of fine-tuning, which is quite common in practice.
Our dataset is hierarchically structured, featuring 1,706 images across five categories: anime characters, movie characters, scenes, stuffed toys, and styles. These categories further break down into various classes and sub-classes. 
Importantly, classes under ``scenes'' and ``stuffed toys'' contain only 4 to 12 images, whereas other categories have 45 to 200 images per class.
%Detailed descriptions and image counts for each are provided in \needref.

The influence of training captions on the fine-tuned model is also widely acknowledged in the community.
It is particularly observed that training with uninformative captions such as \texttt{``A photo of \trainingword''}, which are commonly employed in the literature, can lead to subpar results.
In light of this, we use a publicly available tagger
%\footnote{\url{https://huggingface.co/SmilingWolf/wd-v1-4-convnext-tagger-v2}} 
to tag the training images.
We then remove tags that are inherently tied to each target concept. The resulting tags are combined with the concept descriptor to create more informative captions as \texttt{``\trainingword, \{tag\thinspace 1\}, ..., \{tag\thinspace k\}''}. 
To justify this choice, comparative analyses for models trained using different captions are presented in \cref{apx:captioning}.
%Further details on caption processing and comparative analyses using different captions are described in \needref.



%During training, we repeat each image a number of times within each epoch to ensure images from different classes are seen roughly the same number of times.

\subsection{Algorithm Configuration and Evaluation}
\label{subsec:algo-config-eval}

Our experiments focus on methods that are implemented in the \lycoris~library, and notably \lora, \loha, \lokr, and \nt~(note that DreamBooth \citealp{ruiz2023dreambooth} can be simply regarded as \nt~with regularization images).
For each of these four algorithms, we define a set of default hyperparameters and then individually vary one of the following hyperparameters: learning rate, trained layers, dimension and alpha for \lora~and \loha, and factor for \lokr. This leads to 26 distinct configurations. For each configuration, three models are trained using different random seeds, and three checkpoints are saved along each fine-tuning, giving in this way 234 checkpoints in the end.
While other parameter-efficient fine-tuning methods exist in the literature, most of the proposed modifications are complementary to our approach. We thus do not include them for simplicity.


\paragraph{Data Balancing.}
To address dataset imbalance, we repeat each image a number of times within each epoch to ensure images from different classes are equally exposed during training.

\paragraph{Evaluation Procedure\afterhead}
To evaluate the trained models, we consider the following four types of prompts
\begin{enumerate*}[\itshape i\upshape)]
    \item{} <train> training captions,
    \item{} <trigger> concept descriptor alone,
    \item{} <alter> generalization prompts with content alteration, and
    \item{} <style> generalization prompts with style alteration.
\end{enumerate*}
Using only the concept descriptor tests if the model can accurately reproduce the concept without using the exact training captions.
As for generalization prompts, only a single target concept is involved.
This thus evaluates the model's ability to combine innate and fine-tuned knowledge.
For each considered prompt type, we generate 100 images for each class or sub-class, resulting in a total of 14,900 images per checkpoint.
Note that we do not include concept-agnostic prompts in our experiments.
%Further experimental details are described in \needref.

\paragraph{Evaluation Metrics\afterhead}
Our evaluation metrics are designed to capture the criteria delineated in \cref{subsec:eval-criteria} and are computed on a per (sub)-class basis. We briefly describe below the metrics that are used for each criterion.
% while full details are provided in

\begin{itemize}
\item \textbf{Fidelity}: We assess the similarity between the generated and dataset images using \emph{average cosine similarity} and \emph{squared centroid distance} between their DINOv2 embeddings \citep{oquab2023dinov2}.
\item \textbf{Controllability}: The alignment between generated images and corresponding prompts is measured via \emph{average cosine similarity} in the CLIP feature space \citep{radford2021learning}.
\item \textbf{Diversity}: Diversity of images generated with a single prompt is measured by the \emph{Vendi score} \citep{friedman2023the}, calculated using the DINOv2 embeddings.
\item \textbf{Base Model Preservation}: This generally needs to be evaluated on a case-by-case basis, depending on which aspect of the base model we would like to retain. Specifically, we examine potential style leakage by measuring the standard \emph{style loss}~\citep{johnson2016perceptual} between base and fine-tuned model outputs for <style> prompts.
\item \textbf{Image Quality}: Although numerous methods have been developed for image quality assessment, most of them target natural images.
As far as we are aware, currently, there still lacks a systematic approach for assessing the quality of AI-generated images.
We attempted experiments with three leading pretrained quality assessment models, as detailed in \cref{apx:image-quality-assessment}, but found them unsuitable for our context. 
We thus do not include any quality metrics in our primary experiments.
\end{itemize}

Further justification for our metric choices is provided in \cref{apx:correlation,apx:image-features-classification}, where we compute correlation coefficients for a wider range of metrics and conduct experiments across three classification datasets to assess the sensitivity of different image features to change in a certain image attribute.


\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
    \begin{adjustbox}{valign=t}
    \includegraphics[width=0.29\textwidth]{images/main/people_shap_imsimout.pdf}
    \end{adjustbox}
    \begin{adjustbox}{valign=t}
    \includegraphics[width=0.7\textwidth]{images/main/lr_people_xyplot_step30.pdf}
    \end{adjustbox}
    \caption{Plots for category ``movie characters'' (scatter plots are for 30 epoch checkpoints)}
    \label{subfig:main-people}
    \end{subfigure}
    %
    \\[0.2cm]
    \begin{subfigure}{\textwidth}
    \begin{adjustbox}{valign=t}
    \includegraphics[width=0.29\textwidth]{images/main/scene_shap_imsimout.pdf}
    \end{adjustbox}
    \begin{adjustbox}{valign=t}
    \includegraphics[width=0.7\textwidth]{images/main/capacity_scene_xyplot_step10.pdf}
    \end{adjustbox}
    \caption{Plots for category ``scene'' (scatter plots are for 10 epoch checkpoints)}
    \label{subfig:main-scene}
    \end{subfigure}
    \caption{SHAP beeswarm charts and scatter plots for analyzing the impact of change in different algorithm components.
    In the beeswarm plots, \lora~is in blue, \loha~is in purple, \lokr~is in purple red, and \nt~is in red.
    Model capacity is adjusted by either increasing dimension (for \lora~or \loha) or decreasing factor (for \lokr).
    In the scatter plots, SCD indicates that we use squared centroid distance to measure image similarity.
    This removes the implicit penalization towards more diverse image sets in the computation of average cosine similarity (see \cref{apx:metrics} for details). We believe it is thus more suitable when we are interested in the trade-off between fidelity and diversity.
    The error bars in the scatter plots represent standard errors of the metric values across random seeds and classes.
    }
    \label{fig:plots-main}
    \vspace*{-0.8em}
\end{figure}

\subsection{Experimental Results}
\label{subsec:exp-results}

To carry out the analysis, we first transform each computed metric value into a normalized score, ranging from 0 to 1, based on its relative ranking among all the examined checkpoints (234 in total). A score closer to 1 signifies superior performance. Subsequently, these scores are averaged across sub-classes and classes to generate a set of metrics for each category and individual checkpoint. Alongside the scatter plots, which directly indicate the values of these metrics, we also employ SHAP (SHapley Additive exPlanations) analysis \citep{NIPS2017_8a20a862} in conjunction with CatBoost regressor \citep{prokhorenkova2018catboost} to get a clear visualization of the impact of different algorithm components on the considered metrics, as shown in \cref{fig:plots-main}.
In essence, a SHAP value quantifies the impact of a given feature on the model's output.
For a more exhaustive presentation of the results, readers are referred to \cref{apx:plots}.
%This approach provides a lucid visualization of the influence exerted by various algorithmic components on the metrics under consideration, as illustrated in \cref{fig:plots-main}. In essence, a SHAP value quantifies the impact of a given feature on the model's output, thereby offering interpretability. For a more exhaustive presentation of the results, readers are referred to \cref{apx:plots}.


\subsubsection{Challenges in Algorithm Evaluation}

Before delving into our analysis, it is crucial to acknowledge the complexities inherent in evaluating the performance of fine-tuning algorithms. Specifically, we identify several key challenges, including
\begin{enumerate*}[\itshape i\upshape)]
    \item sensitivity to hyperparameters,
    \item performance discrepancy across concepts,
    \item influence of dataset,
    \item conflicting criteria, and
    \item unreliability of evaluation metrics, among others.
\end{enumerate*}
These challenges are discussed in detail in \cref{apx:evaluation-challenges}.
To mitigate some of these issues, our evaluation encompasses a large number of configurations and includes separate analyses for each category. Additionally, we complement our quantitative metrics with visual inspections conducted throughout our experiments (see \cref{apx:add-qualitative} for an extensive set of qualitative results). Finally, we exercise caution in making any definitive claims, emphasizing that there is no one-size-fits-all solution, and acknowledging that exceptions do exist to our guiding principles.




\subsubsection{Analysis of Results and Insights for Fine-Tuning with \lycoris}
\label{subsubsec:algo-config-impact}

%Based on our experiments, we offer below some general guidelines for fine-tuning with \lycoris. Nevertheless, as previously discussed, these should be taken as starting points, subject to validation in different contexts.

In this part, we delve into a detailed examination of our experimental results, aiming to glean actionable insights for fine-tuning with \lycoris. These insights should not be considered as rigid guidelines, but rather as empirical observations designed to serve as foundational reference points.
%Given the complexities of model fine-tuning, these insights may need to be contextually validated or adjusted based on the specifics of a given application.

\disclose{\paragraph{Number of Training Epochs\afterhead}}{\textbf{Number of Training Epochs\afterhead}}
To analyze the evolution of models over training, we generate images from checkpoints obtained after 10, 30, and 50 epochs of training.
Due to the small number of images available for ``scenes'' and ``stuffed toys'' categories and the use of data balancing, the 30 and 50 epoch checkpoints are almost universally overtrained for these concepts, explaining why increasing training epochs decreases image similarity as shown in the SHAP plot of \cref{subfig:main-scene}.\footnote{We also experimented without data balancing and observed undertraining even after 50 epochs.}
Otherwise, increasing the number of epochs generally improves concept fidelity while compromising text-image alignment, diversity, and base model preservation. Exceptions to this trend exist.
Specifically, we observe the \emph{overfit-then-generalize} phenomenon, as often illustrated through the double descent curve \citep{nakkiran2021deep}, in certain situations. We explore this further in \cref{apx:grokking}.




\disclose{\paragraph{Learning Rate\afterhead}}{\textbf{Learning Rate\afterhead}}
We consider three levels of learning rate,
$5\cdot 10^{-7}$, $10^{-6}$, and $5\cdot 10^{-6}$ for \nt, and
$10^{-4}$, $5\cdot 10^{-4}$, and $10^{-3}$ for the other three algorithms.
Within a reasonable range, increasing the learning rate seems to have the same effect as increasing the number of training epochs. A qualitative example is provided in \cref{fig:qualitative-main}.
It is worth noting, however, that an excessively low learning rate cannot be remedied by simply extending the training duration (see \cref{apx:castle}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{images/main/qualitative-main.png}
    \caption{Qualitative comparison of checkpoints trained with different configurations.
    Samples of the top row are generated using only concept descriptors while samples of the bottom row are generated with the two prompts ``[$V_{\text{castle}}$] scene stands against a backdrop of snow-capped mountains''
    and 
    ``[$V_{\text{castle}}$] scene surrounded by a lush, vibrant forest''.
    The number of training epochs is chosen according to the concept category.
    %More successful generations are marked with red frames.
    }
    \vspace*{-0.8em}
    \label{fig:qualitative-main}
\end{figure}

\disclose{\paragraph{Algorithm\afterhead}}{\textbf{Algorithm\afterhead}}
A central question driving the development of \lycoris~is to assess how different methods of decomposing the model update affect the final model's performance.
We summarize our observations in \cref{tab:methods-performance}.
We distinguish here between two learning rates for \nt~as they result in models that perform very differently.
In particular, \cref{subfig:main-people} reveals that \nt~at a learning rate of $5\cdot 10^{-6}$ achieves high image similarity for training prompts and high text similarity for generalization prompts.
However, this strong performance in text similarity for generalization prompts also leads to a lower image similarity score for these prompts, highlighting in this way both the challenges in comprehensive method comparison and the necessity of evaluating on different types of prompts independently. As for \lora, \loha, and \lokr, comparisons are based on configurations with similar parameter counts and otherwise the same hyperparameters. Style preservation is not included in this comparison as no consistent trend across concept categories is observed.


% From \cref{fig:plots-main}, we observe 
% that among the three decomposition methods, \lokr~appears to outperform \lora, which in turn outperforms \loha, in terms of image similarity, whereas for other metrics,  this ranking is typically reversed. 
% Importantly, these observations are made under conditions where all methods have approximately the same number of parameters, and one should keep in mind that \lokr~has the unique advantage of being adaptable across a wider range of potential parameter counts.
% \cref{subfig:main-people} also reveals that \nt~at a learning rate of $5\cdot 10^{-6}$ achieves high image similarity for training prompts and high text similarity for generalization prompts.
% However, this strong performance in text similarity for generalization prompts leads to a lower image similarity score for these prompts due to the metrics' design.
% This underlines the necessity of evaluating different types of prompts independently.
% Lastly, we find that, \nt, when properly tuned, can achieve high text and image similarity and strong base model style preservation but often compromises diversity.

\disclose{\paragraph{Trained Layers\afterhead}}{\textbf{Trained Layers\afterhead}}
To investigate the effects of fine-tuning different layers, we examine three distinct presets:
\begin{enumerate*}[\itshape i\upshape)]
    \item attn-only: where we only fine-tune attention layers;
    \item attn-mlp: where we fine-tune both attention and feedforward layers; and
    \item full network: where we fine-tune all the layers, including the convolutional ones.
\end{enumerate*}
As can be seen from the SHAP plots in \cref{fig:plots-main}, when all the other parameters are fixed, restricting fine-tuning to only the attention layers leads to a substantial decrease in image similarity while improving other metrics.  This could be unfavorable in certain cases; for instance, the top-left example in \cref{fig:qualitative-main} shows that neglecting to fine-tune the feedforward layers prevents the model from correctly learning the character's uniform.
The impact of fine-tuning the convolutional layers is less discernible, possibly because the metrics we use are not sensitive enough to capture subtle differences.
Overall, our observations align with those made by \cite{han2023svdiff}.
Moreover, it is worth noting that with the ``attn-only'' preset, we also fine-tune the self-attention layers in addition to the cross-attention layers, but this may still be insufficient, as demonstrated above.

\begin{table}[t]
    \centering
    \begin{tabular}{l|ccccc}
    \toprule
        & \lora & \loha & \lokr & Native (lr $5\times10^{-6}$) & Native (lr $10^{-6}$)  \\
    \midrule
        Fidelity & 3 & 2 & 4 & 5 & 1 \\
        Controllability & 2 & 4 & 1 & 3 & 5 \\
        Diversity & 3 & 4 & 2 & 1 & 5 \\
    \bottomrule
    \end{tabular}
    \caption{A tentative ranking based on different evaluation criteria for the methods we explore in our experiments (the \emph{higher} the number, the \emph{better} the method's performance). Although this ranking reflects the general trend observed across different concept categories, deviations of varying degrees are common.
    }
    \label{tab:methods-performance}
    \vspace*{-1.4em}
\end{table}



\disclose{\paragraph{Dimension, Alpha, and Factor\afterhead}}{\textbf{Dimension, Alpha, and Factor\afterhead}}
We finally inspect a number of parameters that are specific to our algorithms---dimension $\dimension$, alpha $\alpha$, and factor $\factor$.
By default, we set the dimension and alpha of \lora~to $8$ and $4$, and of \loha~to $4$ and $2$.
As for \lokr, we set the factor to $8$ and do not perform further decomposition of the second block.
These configurations result in roughly the same parameter counts across the three methods.
To increase model capacity, we either increase the dimension or decrease the factor. This is what we refer to as ``capacity'' in the SHAP plots.
We note that when the ratio between dimension and alpha is fixed, increasing model capacity has roughly the same effect as increasing learning rate or training epochs, though we expect the model's performance could now vary more greatly when varying other hyperparameters.
We especially observe in \cref{subfig:main-scene} that the effects could be reversed when alpha is set to $1$ (these checkpoints are not included in our SHAP analysis).
Some qualitative comparisons are further provided in the bottom row of \cref{fig:qualitative-main}.
