---
weight: 1
bookFlatSection: false
bookToC: true
title: "LoRA Betan√≠t√°si √ötmutat√≥"
summary: "A LoRA Betan√≠t√°si √ötmutat√≥ bemutatja az Alacsony Rang√∫ Adapt√°ci√≥t (LoRA), egy technik√°t a nagy nyelvi √©s diff√∫zi√≥s modellek hat√©kony finomhangol√°s√°ra kis, betan√≠that√≥ alacsony rang√∫ m√°trixok bevezet√©s√©vel az √∂sszes modellparam√©ter m√≥dos√≠t√°sa helyett. Ez a megk√∂zel√≠t√©s az eredeti modell s√∫lyait v√°ltozatlanul hagyja, √©s k√©t tov√°bbi m√°trixot illeszt be minden r√©tegbe a sz√ºks√©ges m√≥dos√≠t√°sok megtanul√°s√°hoz. A LoRA k√∂nny≈±s√∫ly√∫, √≠gy t√∂bb adapt√°ci√≥ betan√≠t√°sa is megval√≥s√≠that√≥ nagy t√°rol√°si k√∂vetelm√©nyek n√©lk√ºl. Az √∫tmutat√≥ √∂sszehasonl√≠tja a LoRA-t a LyCORIS-szal, egy fejlett kiterjeszt√©ssel, amely t√∂bb ir√°ny√≠t√°st √©s rugalmass√°got k√≠n√°l, √©s bemutatja a LoKr-t, amely Kronecker-szorzatokat haszn√°l a m√°trix felbont√°s√°hoz, jav√≠tva a mem√≥riahat√©konys√°got √©s az adapt√°ci√≥s folyamat ir√°ny√≠t√°s√°t."
aliases:
  - /hu/docs/yiff_toolkit/lora_training/
  - /hu/docs/yiff_toolkit/lora_training
  - /hu/docs/yiff_toolkit/Lora Training/
  - /hu/docs/yiff_toolkit/Lora Training
  - /hu/docs/yiff_toolkit/lora training
  - /hu/docs/yiff_toolkit/lora training/
  - /hu/docs/yiff_toolkit/lora_training_guide/
  - /hu/docs/yiff_toolkit/lora_training_guide
  - /hu/docs/yiff_toolkit/Lora Training Guide/
  - /hu/docs/yiff_toolkit/Lora Training Guide
---

<!--markdownlint-disable MD025 MD033 MD034 -->

# LoRA Betan√≠t√°si √ötmutat√≥

---

## Mik azok a LoRA-k?

---

A LoRA (Low-Rank Adaptation - Alacsony Rang√∫ Adapt√°ci√≥) egy olyan technika, amelyet nagy m√©ret≈± nyelvi √©s diff√∫zi√≥s modellek hat√©kony finomhangol√°s√°ra terveztek. Ahelyett, hogy a teljes modellparam√©ter-k√©szletet √°tdolgozn√°k - ami milli√°rdos nagys√°grend≈± lehet - a LoRA kis m√©ret≈±, betan√≠that√≥ "alacsony rang√∫" m√°trixokat vezet be a modell viselked√©s√©nek adapt√°l√°s√°ra. Ezt az innovat√≠v megk√∂zel√≠t√©st a Microsoft kutat√≥i r√©szletezt√©k a ["LoRA: Low-Rank Adaptation of Large Language Models"](https://arxiv.org/abs/2106.09685) c√≠m≈± tanulm√°nyukban.

## Alszekci√≥k

---

{{< section details >}}

## Telep√≠t√©si Tippek

---

El≈ësz√∂r t√∂ltsd le kohya_ss [sd-scripts](https://github.com/kohya-ss/sd-scripts) csomagj√°t. A k√∂rnyezetet vagy √∫gy kell be√°ll√≠tanod, ahogy [itt](https://github.com/kohya-ss/sd-scripts?tab=readme-ov-file#windows-installation) le van √≠rva Windows eset√©n, vagy ha Linuxot vagy Minicond√°t haszn√°lsz Windowson, val√≥sz√≠n≈±leg el√©g okos vagy, hogy magadt√≥l is r√°j√∂jj a telep√≠t√©sre. Mindig aj√°nlott a leg√∫jabb [PyTorch](https://pytorch.org/get-started/locally/) verzi√≥t telep√≠teni abba a virtu√°lis k√∂rnyezetbe, amit haszn√°lni fogsz - ami jelenleg a `2.2.2`. Rem√©lem a j√∂v≈ëbeli √©nemnek gyorsabb PyTorch-a lesz!

Na j√≥, arra az esetre, ha m√©gsem lenn√©l el√©g okos az sd-scripts Miniconda alatti telep√≠t√©s√©hez Windowson, nemr√©g "seg√≠tettem" valakinek, √≠gy el tudom mondani:

```bash
# sd-scripts telep√≠t√©se
git clone https://github.com/kohya-ss/sd-scripts
cd sd-scripts

# Conda k√∂rnyezet l√©trehoz√°sa √©s k√∂vetelm√©nyek telep√≠t√©se
conda create -n sdscripts python=3.10.14
conda activate sdscripts
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
python -m pip install --use-pep517 --upgrade -r requirements.txt
python -m pip install --use-pep517 lycoris_lora
accelerate config
```

Az `accelerate config` t√∂bb k√©rd√©st fog feltenni, mindegyiket el kell olvasnod √©s az igazat kell v√°laszolnod. A legt√∂bb esetben az igazs√°g √≠gy n√©z ki: `This machine, No distributed training, no, no, no, all, fp16`.

Lehet, hogy telep√≠teni szeretn√©d az `xformers`-t vagy a `bitsandbytes`-t is.

```bash
# xformers telep√≠t√©se
# Haszn√°ld ugyanezt a parancsot, csak cser√©ld ki az 'xformers'-t b√°rmilyen m√°s csomagra, amire sz√ºks√©ged van
python -m pip install --use-pep517 xformers

# bitsandbytes telep√≠t√©se windowsra
python -m pip install --use-pep517 bitsandbytes --index-url=https://jllllll.github.io/bitsandbytes-windows-webui
```

---

### Pony Betan√≠t√°s

---

Nem hazudok, kicsit bonyolult mindent elmagyar√°zni. De itt van a legjobb pr√≥b√°lkoz√°som, hogy v√©gigmenjek n√©h√°ny "alapvet≈ë" dolgon √©s szinte minden soron sorrendben.

#### Pony Let√∂lt√©se Diffusers Form√°tumban

A betan√≠t√°shoz a diffusers verzi√≥t haszn√°lom, amit √©n konvert√°ltam, let√∂ltheted `git` seg√≠ts√©g√©vel.

```bash
git clone https://huggingface.co/k4d3/ponydiffusers
```

---

#### Minta Prompt F√°jl

A betan√≠t√°s sor√°n egy minta prompt f√°jlt haszn√°lunk k√©pek mintav√©telez√©s√©hez. Egy minta prompt p√©ld√°ul √≠gy n√©zhet ki Pony eset√©ben:

```py
# anthro female kindred
score_9, score_8_up, score_7_up, score_6_up, rating_explicit, source_furry, solo, female anthro kindred, mask, presenting, white pillow, bedroom, looking at viewer, detailed background, amazing_background, scenery porn, realistic, photo --n low quality, worst quality, blurred background, blurry, simple background --w 1024 --h 1024 --d 1 --l 6.0 --s 40
# anthro female wolf
score_9, score_8_up, score_7_up, score_6_up, rating_explicit, source_furry, solo, anthro female wolf, sexy pose, standing, gray fur, brown fur, canine pussy, black nose, blue eyes, pink areola, pink nipples, detailed background, amazing_background, realistic, photo --n low quality, worst quality, blurred background, blurry, simple background --w 1024 --h 1024 --d 1 --l 6.0 --s 40
```

K√©rlek vedd figyelembe, hogy a minta promptok nem haladhatj√°k meg a 77 tokent, haszn√°lhatod a [Count Tokens in Sample Prompts](https://huggingface.co/k4d3/yiff_toolkit/blob/main/dataset_tools/Count%20Tokens%20in%20Sample%20Prompts.ipynb) eszk√∂zt a [/dataset_tools](https://huggingface.co/k4d3/yiff_toolkit/tree/main/dataset_tools) mapp√°b√≥l a promptjaid elemz√©s√©hez.

Ha t√∂bb GPU-val tan√≠tasz, gy≈ëz≈ëdj meg r√≥la, hogy a promptok teljes sz√°ma marad√©k n√©lk√ºl oszthat√≥ a GPU-k sz√°m√°val, k√ºl√∂nben egy k√°rtya t√©tlen marad.

---

#### Betan√≠t√°si Parancsok

---

##### `accelerate launch`

K√©t GPU eset√©n:

```python
accelerate launch --num_processes=2 --multi_gpu --num_machines=1 --gpu_ids=0,1 --num_cpu_threads_per_process=2  "./sdxl_train_network.py"
```

Egy GPU eset√©n:

```python
accelerate launch --num_cpu_threads_per_process=2 "./sdxl_train_network.py"
```

---

&nbsp;

√âs most n√©zz√ºk √°t az `sd-scripts`-nek √°tadhat√≥ param√©tereket.

&nbsp;

##### `--lowram`

Ha kifutsz a rendszermem√≥ri√°b√≥l, mint ahogy √©n is 2 GPU-val √©s egy nagyon nagy modellel, ami GPU-nk√©nt bet√∂lt≈ëdik, ez az opci√≥ seg√≠t sp√≥rolni egy kicsit √©s kimenthet az OOM pokolb√≥l.

---

##### `--pretrained_model_name_or_path`

A let√∂lt√∂tt checkpoint k√∂nyvt√°ra. Javaslom, hogy z√°rd le az el√©r√©si utat egy `/`-rel, ha helyi diffusers modellt haszn√°lsz. Megadhatsz `.safetensors` vagy `.ckpt` f√°jlt is, ha olyan van!

```python
    --pretrained_model_name_or_path="/ponydiffusers/"
```

---

##### `--output_dir`

Itt ker√ºlnek ment√©sre az √∂sszes mentett epoch vagy l√©p√©s, bele√©rtve az utols√≥t is.

```python
    --output_dir="/output_dir"
```

---

##### `--train_data_dir`

Az adatk√©szletet tartalmaz√≥ k√∂nyvt√°r. Ezt kor√°bban egy√ºtt k√©sz√≠tett√ºk el≈ë.

```python
    --train_data_dir="/training_dir"
```

---

##### `--resolution`

Mindig √°ll√≠tsd be a modell felbont√°s√°nak megfelel≈ëen, ami Pony eset√©ben 1024x1024. Ha nem f√©r bele a VRAM-ba, v√©gs≈ë megold√°sk√©nt cs√∂kkentheted `512,512`-re.

```python
    --resolution="1024,1024"
```

---

##### `--enable_bucket`

K√ºl√∂nb√∂z≈ë t√°rol√≥kat hoz l√©tre a k√ºl√∂nb√∂z≈ë k√©par√°ny√∫ k√©pek el≈ëzetes kategoriz√°l√°s√°val. Ez a technika seg√≠t elker√ºlni az olyan probl√©m√°kat, mint a term√©szetellenes v√°g√°sok, amelyek gyakran el≈ëfordulnak, amikor a modelleket n√©gyzet alak√∫ k√©pek el≈ë√°ll√≠t√°s√°ra tan√≠tj√°k. Ez lehet≈ëv√© teszi olyan k√∂tegek l√©trehoz√°s√°t, ahol minden elem azonos m√©ret≈±, de a k√∂tegek k√©pm√©rete k√ºl√∂nb√∂zhet.

---

##### `--bucket_no_upscale`

A h√°l√≥zat √°ltal feldolgozott k√©pek felbont√°s√°t befoly√°solja a k√©pek felsk√°l√°z√°s√°nak letilt√°s√°val. Ha ez az opci√≥ be van √°ll√≠tva, a h√°l√≥zat csak akkor fogja lekicsiny√≠teni a k√©peket, hogy belef√©rjenek a `self.max_area` √°ltal meghat√°rozott maxim√°lis ter√ºletbe, ha a k√©p $sz√©less√©g \times magass√°g$ √©rt√©ke meghaladja ezt az √©rt√©ket.

1. A `select_bucket` f√ºggv√©ny ellen≈ërzi, hogy sz√ºks√©ges-e a lekicsiny√≠t√©s: Ha a `image_width` √©s `image_height` szorzata nagyobb, mint a `self.max_area`, akkor a k√©p t√∫l nagy, √©s le kell kicsiny√≠teni a k√©par√°ny megtart√°sa mellett.
2. Ezut√°n kisz√°m√≠tja azt a sz√©less√©get √©s magass√°got, amelyre a k√©pet √°t kell m√©retezni √∫gy, hogy az √°tm√©retezett k√©p ter√ºlete ne haladja meg a `self.max_area` √©rt√©ket, √©s a k√©par√°ny megmaradjon.
3. A `round_to_steps` f√ºggv√©ny a kerek√≠t√©shez haszn√°latos, amely az √°tm√©retezett dimenzi√≥kat a `self.reso_steps` legk√∂zelebbi t√∂bbsz√∂r√∂s√©re kerek√≠ti, ami egy param√©ter, amely meghat√°rozza a felbont√°si t√°rol√≥k l√©p√©sk√∂z√©t.
4. A k√≥d √∂sszehasonl√≠tja a sz√©less√©g √©s magass√°g k√©par√°ny√°t a kerek√≠t√©s ut√°n, hogy eld√∂ntse, melyik dimenzi√≥t r√©szes√≠tse el≈ënyben az √°tm√©retez√©s ut√°ni k√©par√°ny-hiba minimaliz√°l√°sa √©rdek√©ben.
5. A kisebb k√©par√°ny-hiba alapj√°n kiv√°lasztja azokat az √°tm√©retezett dimenzi√≥kat, amelyek a legjobban meg≈ërzik a k√©p eredeti k√©par√°ny√°t.

√ñsszefoglalva, a `select_bucket` f√ºggv√©ny biztos√≠tja, hogy amikor lekicsiny√≠t√©sre van sz√ºks√©g, a k√©p olyan dimenzi√≥kra legyen √°tm√©retezve, amelyek a felbont√°si l√©p√©sk√∂z (`self.reso_steps`) t√∂bbsz√∂r√∂sei, √©s a lehet≈ë legk√∂zelebb √°llnak az eredeti k√©par√°nyhoz, an√©lk√ºl, hogy meghaladn√°k a megengedett maxim√°lis ter√ºletet (`self.max_area`). **A felsk√°l√°z√°s nem t√∂rt√©nik meg, ha a** `--bucket_no_upscale` **be van √°ll√≠tva.**

---

##### `--min_bucket_reso` √©s `--max_bucket_reso`

Meghat√°rozza a t√°rol√≥k √°ltal haszn√°lt minim√°lis √©s maxim√°lis felbont√°sokat. Ezek az √©rt√©kek figyelmen k√≠v√ºl maradnak, ha a `--bucket_no_upscale` be van √°ll√≠tva.

```python
    --min_bucket_reso=256 --max_bucket_reso=1024
```

---

##### `--network_alpha`

Meghat√°rozza, hogy a betan√≠tott H√°l√≥zati Rangok k√∂z√ºl h√°ny m√≥dos√≠thatja az alapmodellt.

```python
    --network_alpha=4
```

---

##### `--save_model_as`

Haszn√°lhatod ezt a `ckpt` vagy `safetensors` f√°jlform√°tum megad√°s√°hoz.

```python
    --save_model_as="safetensors"
```

---

##### `--network_module`

Meghat√°rozza, hogy melyik h√°l√≥zati modult fogod betan√≠tani.

```python
    --network_module="lycoris.kohya"
```

---

##### `--network_args`

A h√°l√≥zatnak √°tadott argumentumok.

```python
    --network_args \
               "use_reentrant=False" \
               "preset=full" \
               "conv_dim=256" \
               "conv_alpha=4" \
               "use_tucker=False" \
               "use_scalar=False" \
               "rank_dropout_scale=False" \
               "algo=locon" \
               "train_norm=False" \
               "block_dims=8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8" \
               "block_alphas=0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625" \
```

**N√©zz√ºk meg r√©szletesen!**

---

###### `preset`

A LyCORIS-hoz hozz√°adott [Preset](https://github.com/KohakuBlueleaf/LyCORIS/blob/HEAD/docs/Preset.md)/konfigur√°ci√≥ rendszer a finomabb ir√°ny√≠t√°s √©rdek√©ben.

- `full`
  - alap√©rtelmezett preset, az UNet √©s CLIP √∂sszes r√©teg√©t betan√≠tja.
- `full-lin`
  - `full`, de kihagyja a konvol√∫ci√≥s r√©tegeket.
- `attn-mlp`
  - "kohya preset", az √∂sszes transformer blokkot betan√≠tja.
- `attn-only`
  - csak a figyelmi r√©teg lesz betan√≠tva, sok tanulm√°ny csak a figyelmi r√©teget tan√≠tja.
- `unet-transformer-only`
  - ugyanaz, mint a kohya_ss/sd_scripts kikapcsolt TE-vel, vagy az attn-mlp preset enged√©lyezett train_unet_only-val.
- `unet-convblock-only`
  - csak a ResBlock, UpSample, DownSample lesz betan√≠tva.

---

###### `conv_dim` √©s `conv_alpha`

A konvol√∫ci√≥s dimenzi√≥k a modell konvol√∫ci√≥j√°nak rangj√°hoz kapcsol√≥dnak. Ennek az √©rt√©knek a m√≥dos√≠t√°sa [jelent≈ës hat√°ssal](https://ashejunius.com/alpha-and-dimensions-two-wild-settings-of-training-lora-in-stable-diffusion-d7ad3e3a3b0a) lehet, √©s cs√∂kkent√©se befoly√°solta a k√ºl√∂nb√∂z≈ë LoRA mint√°k k√∂z√∂tti eszt√©tikai k√ºl√∂nbs√©geket. Egy karakter arc√°nak betan√≠t√°s√°hoz `128`-as alfa √©rt√©ket haszn√°ltak, m√≠g Kohaku azt javasolta, hogy ezt √°ll√≠tsuk `1`-re mind a LoCon, mind a LoHa eset√©ben.

```python
conv_block_dims = [conv_dim] * num_total_blocks
conv_block_alphas = [conv_alpha] * num_total_blocks
```

---

###### `module_dropout` √©s `dropout` √©s `rank_dropout`

{{< responsive-svg src="/svg/dropout.svg" alt="Dropout a neur√°lis h√°l√≥zatokban" >}}

A `rank_dropout` a dropout egy form√°ja, amely egy szab√°lyoz√°si technika a neur√°lis h√°l√≥zatokban a t√∫ltanul√°s megakad√°lyoz√°s√°ra √©s az √°ltal√°nos√≠t√°s jav√≠t√°s√°ra. Azonban a hagyom√°nyos dropouttal ellent√©tben, amely v√©letlenszer≈±en null√°ra √°ll√≠tja a bemenetek egy r√©sz√©t, a `rank_dropout` az `lx` bemeneti tenzor rangj√°n m≈±k√∂dik. El≈ësz√∂r egy bin√°ris maszk j√∂n l√©tre ugyanazzal a ranggal, mint az `lx`, ahol minden elem `True` √©rt√©k≈± `1 - rank_dropout` val√≥sz√≠n≈±s√©ggel √©s `False` egy√©bk√©nt. Ezut√°n a `mask` alkalmaz√°sra ker√ºl az `lx`-re, hogy v√©letlenszer≈±en null√°ra √°ll√≠tson n√©h√°ny elemet. A dropout alkalmaz√°sa ut√°n egy sk√°l√°z√°si t√©nyez≈ë ker√ºl alkalmaz√°sra az `lx`-re, hogy kompenz√°lja a kiejtett elemeket. Ez az√©rt t√∂rt√©nik, hogy biztos√≠tsa, hogy az `lx` v√°rhat√≥ √∂sszege ugyanaz maradjon a dropout el≈ëtt √©s ut√°n. A sk√°l√°z√°si t√©nyez≈ë `1.0 / (1.0 - self.rank_dropout)`.

Az√©rt h√≠vj√°k "rang" dropoutnak, mert a bemeneti tenzor rangj√°n m≈±k√∂dik, nem pedig az egyes elemein. Ez k√ºl√∂n√∂sen hasznos lehet olyan feladatokn√°l, ahol a bemenet rangja fontos.

Ha a `rank_dropout` √©rt√©ke `0`, az azt jelenti, hogy nem ker√ºl alkalmaz√°sra dropout az `lx` bemeneti tenzor rangj√°n. A maszk minden eleme `True` √©rt√©k≈± lenne, √©s amikor a maszk alkalmaz√°sra ker√ºl az `lx`-re, minden eleme megmaradna, √©s amikor a sk√°l√°z√°si t√©nyez≈ë alkalmaz√°sra ker√ºl a dropout ut√°n, az √©rt√©ke egyenl≈ë lenne a `self.scale`-lel, mert `1.0 / (1.0 - 0)` az `1`. Alapvet≈ëen, ha ezt `0`-ra √°ll√≠tjuk, az hat√©konyan kikapcsolja a dropout mechanizmust, de m√©g mindig v√©gez n√©h√°ny √©rtelmetlen sz√°m√≠t√°st, √©s nem √°ll√≠thatod None-ra, sz√≥val ha t√©nyleg ki akarod kapcsolni a dropoutokat, egyszer≈±en ne add meg ≈ëket! üòá

```python
def forward(self, x):
    org_forwarded = self.org_forward(x)

    # modul dropout
    if self.module_dropout is not None and self.training:
        if torch.rand(1) < self.module_dropout:
            return org_forwarded

    lx = self.lora_down(x)

    # norm√°l dropout
    if self.dropout is not None and self.training:
        lx = torch.nn.functional.dropout(lx, p=self.dropout)

    # rang dropout
    if self.rank_dropout is not None and self.training:
        mask = torch.rand((lx.size(0), self.lora_dim), device=lx.device) > self.rank_dropout
        if len(lx.size()) == 3:
            mask = mask.unsqueeze(1)
        elif len(lx.size()) == 4:
            mask = mask.unsqueeze(-1).unsqueeze(-1)
        lx = lx * mask

        scale = self.scale * (1.0 / (1.0 - self.rank_dropout))
    else:
        scale = self.scale

    lx = self.lora_up(lx)

    return org_forwarded + lx * self.multiplier * scale
```

A betan√≠tand√≥ h√°l√≥zatnak t√°mogatnia kell ezt! Tov√°bbi r√©szletek√©rt l√°sd a [PR#545](https://github.com/kohya-ss/sd-scripts/pull/545) pull requestet.

---

###### `use_tucker`

Minden algoritmushoz haszn√°lhat√≥, kiv√©ve az `(IA)^3`-at √©s a nat√≠v finomhangol√°st.

A Tucker-dekompoz√≠ci√≥ egy matematikai m√≥dszer, amely egy tenzort m√°trixok halmaz√°ra √©s egy kis mag tenzorra bont, cs√∂kkentve a modell sz√°m√≠t√°si komplexit√°s√°t √©s mem√≥riaig√©ny√©t. K√ºl√∂nb√∂z≈ë LyCORIS modulokban haszn√°lj√°k k√ºl√∂nb√∂z≈ë blokkokon. A LoCon eset√©ben p√©ld√°ul, ha a `use_tucker` `True` √©s a kernel m√©ret `k_size` nem `(1, 1)`, akkor a konvol√∫ci√≥s m≈±velet h√°rom k√ºl√∂n m≈±veletre bomlik:

1. Egy 1x1-es konvol√∫ci√≥, amely cs√∂kkenti a csatorn√°k sz√°m√°t `in_dim`-r≈ël `lora_dim`-re.
2. Egy konvol√∫ci√≥ az eredeti kernel m√©rettel `k_size`, l√©p√©sk√∂zzel `stride`, √©s paddingel `padding`, de cs√∂kkentett sz√°m√∫ csatorn√°val `lora_dim`.
3. Egy 1x1-es konvol√∫ci√≥, amely visszan√∂veli a csatorn√°k sz√°m√°t `lora_dim`-r≈ël `out_dim`-re.

Ha a `use_tucker` `False` vagy nincs be√°ll√≠tva, vagy ha a kernel m√©ret `k_size` `(1, 1)`, akkor egy standard konvol√∫ci√≥s m≈±velet ker√ºl v√©grehajt√°sra az eredeti kernel m√©rettel, l√©p√©sk√∂zzel √©s paddingel, √©s a csatorn√°k sz√°ma `in_dim`-r≈ël `lora_dim`-re cs√∂kken.

---

###### `use_scalar`

Egy tov√°bbi tanulhat√≥ param√©ter, amely sk√°l√°zza az alacsony rang√∫ s√∫lyok hozz√°j√°rul√°s√°t, miel≈ëtt azok hozz√°ad√≥dn√°nak az eredeti s√∫lyokhoz. Ez a skal√°r szab√°lyozhatja, hogy az alacsony rang√∫ adapt√°ci√≥ mennyire m√≥dos√≠tja az eredeti s√∫lyokat. A skal√°r tan√≠t√°s√°val a modell megtanulhatja az optim√°lis egyens√∫lyt az eredeti el≈ëtan√≠tott s√∫lyok meg≈ërz√©se √©s az alacsony rang√∫ adapt√°ci√≥ enged√©lyez√©se k√∂z√∂tt.

```python
# Ellen≈ërizz√ºk, hogy a 'use_scalar' flag be van-e √°ll√≠tva True-ra
if use_scalar:
    # Ha igen, inicializ√°lunk egy tanulhat√≥ 'scalar' param√©tert 0.0 kezd≈ë√©rt√©kkel.
    # Ez a param√©ter optimaliz√°l√≥dik a tan√≠t√°si folyamat sor√°n.
    self.scalar = nn.Parameter(torch.tensor(0.0))
else:
    # Ha a 'use_scalar' flag False, a 'scalar' √©rt√©ke fix 1.0 lesz.
    # Ez azt jelenti, hogy az alacsony rang√∫ s√∫lyok sk√°l√°z√°s n√©lk√ºl ad√≥dnak hozz√° az eredeti s√∫lyokhoz.
    self.scalar = torch.tensor(1.0)
```

A `use_scalar` flag lehet≈ëv√© teszi a modell sz√°m√°ra, hogy meghat√°rozza, mennyi befoly√°sa legyen az alacsony rang√∫ s√∫lyoknak a v√©gs≈ë s√∫lyokra. Ha a `use_scalar` `True`, a modell megtanulhatja a `self.scalar` optim√°lis √©rt√©k√©t a tan√≠t√°s sor√°n, amely megszorozza az alacsony rang√∫ s√∫lyokat, miel≈ëtt azok hozz√°ad√≥dn√°nak az eredeti s√∫lyokhoz. Ez lehet≈ës√©get biztos√≠t az eredeti el≈ëtan√≠tott s√∫lyok √©s az √∫j alacsony rang√∫ adapt√°ci√≥k k√∂z√∂tti egyens√∫ly megteremt√©s√©re, potenci√°lisan jobb teljes√≠tm√©nyhez √©s hat√©konyabb tan√≠t√°shoz vezetve. A `self.scalar` `0.0` kezd≈ë√©rt√©ke azt sugallja, hogy a modell az alacsony rang√∫ s√∫lyok n√©lk√ºl kezd, √©s a tan√≠t√°s sor√°n tanulja meg a megfelel≈ë sk√°l√°t.

---

###### `rank_dropout_scale`

Egy logikai flag, amely meghat√°rozza, hogy a dropout maszkot √∫gy sk√°l√°zzuk-e, hogy √°tlag√©rt√©ke `1` legyen vagy sem. Ez k√ºl√∂n√∂sen hasznos, amikor meg szeretn√©nk ≈ërizni a tenzor √©rt√©keinek eredeti sk√°l√°j√°t a dropout alkalmaz√°sa ut√°n, ami fontos lehet a tan√≠t√°si folyamat stabilit√°sa szempontj√°b√≥l.

```python
def forward(self, orig_weight, org_bias, new_weight, new_bias, *args, **kwargs):
    # Lek√©rj√ºk az 'oft_blocks' tenzor eszk√∂z√©t. Ez biztos√≠tja, hogy minden √∫j tenzor ugyanazon az eszk√∂z√∂n j√∂jj√∂n l√©tre.
    device = self.oft_blocks.device

    # Ellen≈ërizz√ºk, hogy a rang dropout enged√©lyezve van-e √©s a modell tan√≠t√°si m√≥dban van-e.
    if self.rank_dropout and self.training:
        # L√©trehozunk egy v√©letlenszer≈± tenzort ugyanolyan alakkal, mint az 'oft_blocks', egyenletes eloszl√°sb√≥l vett √©rt√©kekkel.
        # Ezut√°n l√©trehozunk egy dropout maszkot annak ellen≈ërz√©s√©vel, hogy minden √©rt√©k kisebb-e, mint a 'self.rank_dropout' val√≥sz√≠n≈±s√©g.
        drop = (torch.rand(self.oft_blocks, device=device) < self.rank_dropout).to(
            self.oft_blocks.dtype
        )

        # Ha a 'rank_dropout_scale' True, a dropout maszkot √∫gy sk√°l√°zzuk, hogy √°tlag√©rt√©ke 1 legyen.
        # Ez seg√≠t meg≈ërizni a tenzor √©rt√©keinek sk√°l√°j√°t a dropout alkalmaz√°sa ut√°n.
        if self.rank_dropout_scale:
            drop /= drop.mean()
    else:
        # Ha a rang dropout nincs enged√©lyezve vagy a modell nincs tan√≠t√°si m√≥dban, a 'drop' √©rt√©ke 1 (nincs dropout).
        drop = 1
```

---

###### `algo`

A haszn√°lt LyCORIS algoritmus. Megtal√°lhatod a megval√≥s√≠tott algoritmusok [list√°j√°t](https://github.com/KohakuBlueleaf/LyCORIS/blob/HEAD/docs/Algo-List.md) √©s [magyar√°zat√°t](https://github.com/KohakuBlueleaf/LyCORIS/blob/HEAD/docs/Algo-Details.md), egy [dem√≥val](https://github.com/KohakuBlueleaf/LyCORIS/blob/HEAD/docs/Demo.md) egy√ºtt, valamint bele√°shatsz a [kutat√°si tanulm√°nyba](https://arxiv.org/pdf/2309.14859.pdf) is.

---

###### `train_norm`

Szab√°lyozza, hogy betan√≠tsuk-e a normaliz√°ci√≥s r√©tegeket, amelyeket minden algoritmus haszn√°l, kiv√©ve az `(IA)^3`-at.

---

###### `block_dims`

Meghat√°rozza minden blokk rangj√°t, pontosan 25 sz√°mot vesz fel, ez√©rt olyan hossz√∫ ez a sor.

---

###### `block_alphas`

Meghat√°rozza minden blokk alf√°j√°t, ez is pontosan 25 sz√°mot vesz fel, ha nem adod meg, akkor helyette a `network_alpha` √©rt√©ke lesz haszn√°lva.

---

Ezzel befejezt√ºk a `network_args` ismertet√©s√©t.

---

##### `--network_dropout`

Ez a lebeg≈ëpontos sz√°m szab√°lyozza a neuronok kies√©s√©t a tan√≠t√°sb√≥l minden l√©p√©sben, `0` vagy `None` az alap√©rtelmezett viselked√©s (nincs dropout), 1 az √∂sszes neuront kiejten√©. A `weight_decompose=True` haszn√°lata figyelmen k√≠v√ºl hagyja a `network_dropout`-ot, √©s csak a rang √©s modul dropout lesz alkalmazva.

```python
    --network_dropout=0 \
```

---

##### `--lr_scheduler`

A PyTorch-ban a tanul√°si r√°ta √ºtemez≈ë egy olyan eszk√∂z, amely a tanul√°si r√°t√°t a tan√≠t√°si folyamat sor√°n √°ll√≠tja. A modell teljes√≠tm√©ny√©nek f√ºggv√©ny√©ben modul√°lja a tanul√°si r√°t√°t, ami jobb teljes√≠tm√©nyhez √©s r√∂videbb tan√≠t√°si id≈ëh√∂z vezethet.

Lehets√©ges √©rt√©kek: `linear`, `cosine`, `cosine_with_restarts`, `polynomial`, `constant` (alap√©rtelmezett), `constant_with_warmup`, `adafactor`

Megjegyz√©s: az `adafactor` √ºtemez≈ë csak az `adafactor` optimaliz√°l√≥val haszn√°lhat√≥!

```python
    --lr_scheduler="cosine" \
```

---

##### `--lr_scheduler_num_cycles`

Az √∫jraind√≠t√°sok sz√°ma a cosine √ºtemez≈ën√©l √∫jraind√≠t√°sokkal. M√°s √ºtemez≈ë nem haszn√°lja.

```py
    --lr_scheduler_num_cycles=1 \
```

---

##### `--learning_rate` √©s `--unet_lr` √©s `--text_encoder_lr`

A tanul√°si r√°ta hat√°rozza meg, hogy a h√°l√≥zat s√∫lyai mennyire m√≥dosulnak a becs√ºlt hiba alapj√°n minden alkalommal, amikor a s√∫lyok friss√ºlnek. Ha a tanul√°si r√°ta t√∫l nagy, a s√∫lyok t√∫ll√©phetik az optim√°lis megold√°st. Ha t√∫l kicsi, a s√∫lyok elakadhatnak egy szuboptim√°lis megold√°sban.

Az AdamW eset√©ben az optim√°lis LR `0.0001` vagy `1e-4` szokott lenni, ha le akarod ny≈±g√∂zni a bar√°taidat.

```py
    --learning_rate=0.0001 --unet_lr=0.0001 --text_encoder_lr=0.0001
```

---

##### `--network_dim`

A H√°l√≥zati Rang (Dimenzi√≥) felel≈ës az√©rt, hogy h√°ny jellemz≈ët fog a LoRA betan√≠tani. Szoros kapcsolatban √°ll a H√°l√≥zati Alf√°val √©s az Unet + TE tanul√°si r√°t√°kkal, valamint term√©szetesen az adatk√©szlet min≈ës√©g√©vel. Er≈ësen aj√°nlott szem√©lyes k√≠s√©rletez√©s ezekkel az √©rt√©kekkel.

```py
    --network_dim=8
```

---

##### `--output_name`

Add meg a kimeneti nevet a f√°jlkiterjeszt√©s n√©lk√ºl.

**FIGYELMEZTET√âS**: Ha valamilyen okb√≥l ez valaha √ºresen marad, az utols√≥ epoch nem lesz elmentve!

```py
    --output_name="last"
```

---

##### `--scale_weight_norms`

A max-norm regulariz√°ci√≥ egy olyan technika, amely korl√°tozza a bej√∂v≈ë s√∫lyvektorok norm√°j√°t minden rejtett egys√©gn√©l egy r√∂gz√≠tett konstans fels≈ë hat√°rral. Megakad√°lyozza, hogy a s√∫lyok t√∫l nagyra n≈ëjenek, √©s seg√≠t jav√≠tani a sztochasztikus gradiens ereszked√©s teljes√≠tm√©ny√©t a m√©ly neur√°lis h√°l√≥k tan√≠t√°sa sor√°n.

A Dropout a h√°l√≥zat architekt√∫r√°j√°t √©rinti a s√∫lyok megv√°ltoztat√°sa n√©lk√ºl, m√≠g a Max-Norm Regulariz√°ci√≥ k√∂zvetlen√ºl m√≥dos√≠tja a h√°l√≥zat s√∫lyait. Mindk√©t technik√°t a t√∫ltanul√°s megakad√°lyoz√°s√°ra √©s a modell √°ltal√°nos√≠t√°s√°nak jav√≠t√°s√°ra haszn√°lj√°k. Mindkett≈ër≈ël t√∂bbet tanulhatsz ebben a [kutat√°si tanulm√°nyban](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf).

```py
    --scale_weight_norms=1.0
```

---

##### `--max_grad_norm`

M√°s n√©ven Gradiens V√°g√°s, ha azt veszed √©szre, hogy a gradiensek felrobbannak a tan√≠t√°s sor√°n (a vesztes√©g NaN lesz vagy nagyon nagy), fontold meg a `--max_grad_norm` param√©ter be√°ll√≠t√°s√°t. Ez a gradienseken m≈±k√∂dik a visszaterjeszt√©si folyamat sor√°n, m√≠g a `--scale_weight_norms` a neur√°lis h√°l√≥zat s√∫lyain m≈±k√∂dik. Ez lehet≈ëv√© teszi, hogy kieg√©sz√≠ts√©k egym√°st √©s robusztusabb megk√∂zel√≠t√©st ny√∫jtsanak a tanul√°si folyamat stabiliz√°l√°s√°hoz √©s a modell teljes√≠tm√©ny√©nek jav√≠t√°s√°hoz.

```py
    --max_grad_norm=1.0
```

---

##### `--no_half_vae`

Kikapcsolja a kevert pontoss√°got az SDXL VAE eset√©ben √©s `float32`-re √°ll√≠tja. Nagyon hasznos, ha nem szereted a NaN-okat.

---

##### `--save_every_n_epochs` √©s `--save_last_n_epochs` vagy `--save_every_n_steps` √©s `--save_last_n_steps`

- `--save_every_n_steps` √©s `--save_every_n_epochs`: Egy LoRA f√°jl j√∂n l√©tre minden n-edik l√©p√©sn√©l vagy epochn√°l, amit itt megadsz.
- `--save_last_n_steps` √©s `--save_last_n_epochs`: Eldobja az √∂sszes mentett f√°jlt, kiv√©ve az utols√≥ `n` darabot, amit itt megadsz.

A tanul√°s mindig azzal fog v√©get √©rni, amit a `--max_train_epochs` vagy `--max_train_steps` param√©terben megadsz.

```py
    --save_every_n_epochs=50
```

---

##### `--mixed_precision`

Ez a be√°ll√≠t√°s hat√°rozza meg a sz√°m√≠t√°si pontoss√°got a tan√≠t√°s sor√°n. A kevert pontoss√°g v√°laszt√°sa felgyors√≠thatja a tan√≠t√°st √©s cs√∂kkentheti a mem√≥riahaszn√°latot, de potenci√°lis numerikus instabilit√°st okozhat. √çme a lehet≈ës√©gek √©s azok el≈ënyei-h√°tr√°nyai:

- "no": Teljes 32-bites pontoss√°got haszn√°l. Lassabb, de stabilabb.
- "fp16": 16-bites pontoss√°got haszn√°l ahol lehets√©ges, visszaesik 32-bitre amikor sz√ºks√©ges. Ez felgyors√≠thatja a tan√≠t√°st √©s cs√∂kkentheti a mem√≥riahaszn√°latot, de id≈ënk√©nt numerikus instabilit√°shoz vezethet.
- "bf16": bfloat16 pontoss√°got haszn√°l. J√≥ egyens√∫lyt k√≠n√°l a 32-bites lebeg≈ëpontosok tartom√°nya √©s a 16-bites lebeg≈ëpontosok mem√≥riamegtakar√≠t√°sa k√∂z√∂tt.

V√°lassz b√∂lcsen a hardvered k√©pess√©gei √©s a stabilit√°si k√∂vetelm√©nyek alapj√°n. Ha NaN vesztes√©geket vagy m√°s numerikus probl√©m√°kat tapasztalsz a tan√≠t√°s sor√°n, fontold meg a teljes pontoss√°gra v√°lt√°st vagy m√°s hiperparam√©terek m√≥dos√≠t√°s√°t.

```py
    --mixed_precision="bf16"
```

---

##### `--save_precision`

Ez a param√©ter hat√°rozza meg a mentett modells√∫lyok pontoss√°g√°t. Ez egy kulcsfontoss√°g√∫ v√°laszt√°s, amely mind a f√°jlm√©retet, mind a betan√≠tott LoRA pontoss√°g√°t befoly√°solja. √çme, amit tudnod kell:

- "fp32": Teljes 32-bites pontoss√°g. Ez a legpontosabb, de t√∂bb t√°rol√≥helyet foglal.
- "fp16": 16-bites pontoss√°g. J√≥ egyens√∫ly a pontoss√°g √©s a f√°jlm√©ret k√∂z√∂tt, a legt√∂bb esetben megfelel≈ë.
- "bf16": bfloat16 pontoss√°g. Sz√©lesebb tartom√°nyt k√≠n√°l, mint az fp16, de kevesebb pontoss√°ggal, bizonyos hardverkonfigur√°ci√≥kn√°l hasznos.

V√°lassz a t√°rol√°si korl√°taid √©s pontoss√°gi k√∂vetelm√©nyeid alapj√°n. Ha nem vagy biztos benne, az "fp16" egy megb√≠zhat√≥ alap√©rtelmezett √©rt√©k, amely a legt√∂bb helyzetben j√≥l m≈±k√∂dik. √âsszer≈± m√©ret≈± LoRA f√°jlt eredm√©nyez t√∫l sok pontoss√°g fel√°ldoz√°sa n√©lk√ºl.

```py
    --save_precision="fp16"
```

##### `--caption_extension`

A feliratf√°jlok kiterjeszt√©se. Az alap√©rtelmezett `.caption`. Ezek a feliratf√°jlok olyan sz√∂veges le√≠r√°sokat tartalmaznak, amelyek a tan√≠t√°si k√©pekhez kapcsol√≥dnak. Amikor futtatod a tan√≠t√°si szkriptet, az a megadott kiterjeszt√©s≈± f√°jlokat fogja keresni a tan√≠t√°si adatok mapp√°j√°ban. A szkript ezeknek a f√°jloknak a tartalm√°t haszn√°lja feliratk√©nt, hogy kontextust biztos√≠tson a k√©pekhez a tan√≠t√°si folyamat sor√°n.

P√©ld√°ul, ha a k√©peid neve `image1.jpg`, `image2.jpg`, √©s √≠gy tov√°bb, √©s az alap√©rtelmezett .caption kiterjeszt√©st haszn√°lod, a szkript azt v√°rja, hogy a feliratf√°jlok neve `image1.caption`, `image2.caption`, stb. legyen. Ha m√°s kiterjeszt√©st szeretn√©l haszn√°lni, p√©ld√°ul `.txt`-t, akkor a caption_extension param√©tert `.txt`-re √°ll√≠tan√°d, √©s a szkript ekkor `image1.txt`, `image2.txt`, √©s √≠gy tov√°bb f√°jlokat keresne.

```py
    --caption_extension=".txt"
```

##### `--cache_latents` √©s `--cache_latents_to_disk`

Ez a k√©t param√©ter egy√ºttm≈±k√∂dik a mem√≥riahaszn√°lat optimaliz√°l√°s√°ban √©s potenci√°lisan felgyors√≠thatja a tan√≠t√°st:

- `--cache_latents`: Ez az opci√≥ a mem√≥ri√°ban t√°rolja a tan√≠t√°si k√©pek l√°tens reprezent√°ci√≥it. Ez√°ltal a modellnek nem kell minden tan√≠t√°si l√©p√©sn√©l √∫jra k√≥dolnia a k√©peket l√°tens t√©rbe, ami jelent≈ësen felgyors√≠thatja a tan√≠t√°st, k√ºl√∂n√∂sen nagyobb adatk√©szletek eset√©n.

- `--cache_latents_to_disk`: A `--cache_latents`-szel egy√ºtt haszn√°lva ez az opci√≥ lehet≈ëv√© teszi, hogy a gyors√≠t√≥t√°razott l√°tens reprezent√°ci√≥k a lemezen t√°rol√≥djanak ahelyett, hogy mind a mem√≥ri√°ban maradn√°nak. Ez k√ºl√∂n√∂sen hasznos, ha olyan nagy adatk√©szleted van, amely meghaladja a rendelkez√©sre √°ll√≥ RAM-ot.

Ezeknek az opci√≥knak a haszn√°lata t√∂bb el≈ënnyel j√°rhat:

1. Gyorsabb tan√≠t√°s: A l√°tens reprezent√°ci√≥k el≈ëzetes kisz√°m√≠t√°s√°val √©s gyors√≠t√≥t√°raz√°s√°val cs√∂kkented a sz√°m√≠t√°si terhel√©st minden tan√≠t√°si l√©p√©sn√©l.
2. Cs√∂kkentett VRAM haszn√°lat: A lemezre t√∂rt√©n≈ë gyors√≠t√≥t√°raz√°s seg√≠thet hat√©konyabban kezelni a mem√≥ri√°t, k√ºl√∂n√∂sen nagy adatk√©szletek eset√©n.
3. Konzisztencia: Az el≈ëre kisz√°m√≠tott l√°tens reprezent√°ci√≥k biztos√≠tj√°k, hogy ugyanaz a l√°tens reprezent√°ci√≥ legyen haszn√°lva minden k√©phez az epochok sor√°n, ami stabilabb tan√≠t√°shoz vezethet.

Azonban vedd figyelembe, hogy a l√°tens reprezent√°ci√≥k gyors√≠t√≥t√°raz√°sa jelent≈ës lemezter√ºletet haszn√°lhat, k√ºl√∂n√∂sen nagy adatk√©szletek eset√©n. Gy≈ëz≈ëdj meg r√≥la, hogy elegend≈ë t√°rhely √°ll rendelkez√©sre, amikor a `--cache_latents_to_disk` opci√≥t haszn√°lod.

```py
    --cache_latents --cache_latents_to_disk
```

---

##### `--optimizer_type`

Az alap√©rtelmezett optimaliz√°l√≥ az `AdamW`, √©s havonta vagy √∫gy k√∂r√ºlbel√ºl egy csom√≥ √∫j ker√ºl hozz√°ad√°sra, ez√©rt nem sorolom fel mindet, megtal√°lhatod a list√°t, ha t√©nyleg akarod, de az `AdamW` a legjobb e sorok √≠r√°sakor, √≠gy ezt haszn√°ljuk!

```py
    --optimizer_type="AdamW"
```

---

##### `--dataset_repeats`

Megism√©tli az adatk√©szletet feliratokkal t√∂rt√©n≈ë tan√≠t√°skor, alap√©rtelmezetten `1`-re van √°ll√≠tva, √≠gy mi ezt `0`-ra √°ll√≠tjuk:

```py
    --dataset_repeats=0
```

---

##### `--max_train_steps`

Add meg a tan√≠t√°si l√©p√©sek vagy epochok sz√°m√°t. Ha mind a `--max_train_steps`, mind a `--max_train_epochs` meg van adva, az epochok sz√°ma √©lvez els≈ëbbs√©get.

```py
    --max_train_steps=400
```

---

##### `--shuffle_caption`

√ñsszekeveri a `--caption_separator` √°ltal meghat√°rozott feliratokat, alap√©rtelmezetten ez egy vessz≈ë `,`, ami t√∂k√©letesen m≈±k√∂dik a mi eset√ºnkben, mivel a felirataink √≠gy n√©znek ki:

> rating_questionable, 5 fingers, anthro, bent over, big breasts, blue eyes, blue hair, breasts, butt, claws, curved horn, female, finger claws, fingers, fur, hair, huge breasts, looking at viewer, looking back, looking back at viewer, nipples, nude, pink body, pink hair, pink nipples, rear view, solo, tail, tail tuft, tuft, by lunarii, by x-leon-x, mythology, krystal \(darkmaster781\), dragon, scalie, wickerbeast, The image showcases a pink-scaled wickerbeast a furred dragon creature with blue eyes., She has large breasts and a thick tail., Her blue and pink horns are curved and pointy and she has a slight smiling expression on her face., Her scales are shiny and she has a blue and pink pattern on her body., Her hair is a mix of pink and blue., She is looking back at the viewer with a curious expression., She has a slight blush.,

Mint l√°thatod, nemcsak a c√≠mk√©ket, hanem a felirat r√©szt is vessz≈ëvel v√°lasztottam el, hogy minden √∂sszekeveredjen.

MEGJEGYZ√âS: A `--cache_text_encoder_outputs` √©s a `--cache_text_encoder_outputs_to_disk` nem haszn√°lhat√≥ egy√ºtt a `--shuffle_caption`-nel. Mindkett≈ë a VRAM haszn√°lat cs√∂kkent√©s√©t c√©lozza, neked kell eld√∂ntened, melyiket v√°lasztod!

---

##### `--sdpa` vagy `--xformers` vagy `--mem_eff_attn`

Mindegyik opci√≥ m√≥dos√≠tja a modellben haszn√°lt figyelmi mechanizmust, ami jelent≈ës hat√°ssal lehet a modell teljes√≠tm√©ny√©re √©s mem√≥riahaszn√°lat√°ra. A v√°laszt√°s az `--xformers` vagy `--mem_eff_attn` √©s az `--spda` k√∂z√∂tt a GPU-dt√≥l f√ºgg. Tesztelheted ≈ëket egy tan√≠t√°s megism√©tl√©s√©vel!

- `--xformers`: Ez a flag enged√©lyezi az XFormers haszn√°lat√°t a modellben. Az XFormers egy Facebook Research √°ltal fejlesztett k√∂nyvt√°r, amely k√ºl√∂nb√∂z≈ë hardverekre √©s felhaszn√°l√°si esetekre optimaliz√°lt transformer modellek gy≈±jtem√©ny√©t biztos√≠tja. Ezek a modellek √∫gy vannak tervezve, hogy nagyon hat√©konyak, rugalmasak √©s testreszabhat√≥ak legyenek. K√ºl√∂nf√©le figyelmi mechanizmusokat √©s egy√©b funkci√≥kat k√≠n√°lnak, amelyek hasznosak lehetnek olyan helyzetekben, ahol korl√°tozott GPU mem√≥ri√°val rendelkezel vagy nagy m√©ret≈± adatokat kell kezelned.
- `--mem_eff_attn`: Ez a flag enged√©lyezi a mem√≥ria-hat√©kony figyelmi mechanizmusok haszn√°lat√°t a modellben. A mem√≥ria-hat√©kony figyelem √∫gy van tervezve, hogy cs√∂kkentse a mem√≥riahaszn√°latot a transformer modellek tan√≠t√°sa sor√°n, ami k√ºl√∂n√∂sen hasznos lehet nagy modellek vagy adatk√©szletek eset√©n.
- `--sdpa`: Ez az opci√≥ enged√©lyezi a Sk√°l√°zott Skal√°ris Szorzat Figyelem (SDPA) haszn√°lat√°t a modellben. Az SDPA a transformer modellek alapvet≈ë komponense, amely kisz√°m√≠tja a figyelmi pontsz√°mokat a lek√©rdez√©sek √©s kulcsok k√∂z√∂tt. A skal√°ris szorzatokat a kulcsok dimenzi√≥j√°val sk√°l√°zza a gradiensek stabiliz√°l√°sa √©rdek√©ben a tan√≠t√°s sor√°n. Ez a mechanizmus k√ºl√∂n√∂sen hasznos hossz√∫ sorozatok kezel√©s√©re √©s potenci√°lisan jav√≠thatja a modell k√©pess√©g√©t a hossz√∫ t√°v√∫ f√ºgg≈ës√©gek megragad√°s√°ra.

```python
    --sdpa
```

---

##### `--multires_noise_iterations` √©s `--multires_noise_discount`

A t√∂bbfelbont√°s√∫ zaj egy √∫j megk√∂zel√≠t√©s, amely t√∂bb felbont√°sban ad zajt egy k√©phez vagy l√°tens k√©phez a diff√∫zi√≥s modellek tan√≠t√°sa sor√°n. Az ezzel a technik√°val tan√≠tott modell vizu√°lisan leny≈±g√∂z≈ë k√©peket gener√°lhat, amelyek eszt√©tikailag k√ºl√∂nb√∂znek a diff√∫zi√≥s modellek szok√°sos kimeneteit≈ël.

A t√∂bbfelbont√°s√∫ zajjal tan√≠tott modell v√°ltozatosabb k√©peket tud gener√°lni, mint a hagyom√°nyos stabil diff√∫zi√≥, bele√©rtve a rendk√≠v√ºl vil√°gos vagy s√∂t√©t k√©peket is. Ezek t√∂rt√©nelmileg nehezen voltak el√©rhet≈ëk an√©lk√ºl, hogy nagy sz√°m√∫ mintav√©teli l√©p√©st haszn√°ltunk volna.

Ez a technika k√ºl√∂n√∂sen el≈ëny√∂s kis adatk√©szletekkel val√≥ munka sor√°n, de szerintem soha nem kellene nem haszn√°lni.

A `--multires_noise_discount` param√©ter szab√°lyozza, hogy mennyire gyeng√ºl a zaj mennyis√©ge minden felbont√°sn√°l. A 0.1 √©rt√©k aj√°nlott. A `--multires_noise_iterations` param√©ter hat√°rozza meg a t√∂bbfelbont√°s√∫ zaj hozz√°ad√°s√°nak iter√°ci√≥sz√°m√°t, 6-10 k√∂z√∂tti aj√°nlott tartom√°nnyal.

K√©rlek vedd figyelembe, hogy a `--multires_noise_discount`-nak nincs hat√°sa a `--multires_noise_iterations` n√©lk√ºl.

###### Implement√°ci√≥s R√©szletek

A `get_noise_noisy_latents_and_timesteps` f√ºggv√©ny mintav√©telezi a zajt, amely hozz√°ad√≥dik a l√°tens reprezent√°ci√≥khoz. Ha az `args.noise_offset` igaz, zajeltol√°st alkalmaz. Ha az `args.multires_noise_iterations` igaz, t√∂bbfelbont√°s√∫ zajt alkalmaz a mintav√©telezett zajra.

A f√ºggv√©ny ezut√°n minden k√©phez v√©letlenszer≈± id≈ël√©p√©st mintav√©telez, √©s zajt ad a l√°tens reprezent√°ci√≥khoz az egyes id≈ël√©p√©sekn√©l l√©v≈ë zajmagnitude szerint. Ez az el≈ëre ir√°nyul√≥ diff√∫zi√≥s folyamat.

A `pyramid_noise_like` f√ºggv√©ny piramis szerkezet≈± zajt gener√°l. Az eredeti zajjal kezdi, √©s felsk√°l√°zott zajt ad hozz√° cs√∂kken≈ë felbont√°sokban. A zaj minden szinten egy diszkontfaktorral sk√°l√°z√≥dik, amely a szint hatv√°ny√°ra van emelve. A zaj ezut√°n visszask√°l√°z√≥dik k√∂r√ºlbel√ºl egys√©gnyi varianci√°ra. Ezt a f√ºggv√©nyt haszn√°lj√°k a t√∂bbfelbont√°s√∫ zaj megval√≥s√≠t√°s√°hoz.

```python
    --multires_noise_iterations=10 --multires_noise_discount=0.1
```

---

##### `--sample_prompts` √©s `--sample_sampler` √©s `--sample_every_n_steps`

Lehet≈ës√©ged van k√©peket gener√°lni a tan√≠t√°s sor√°n, hogy ellen≈ërizhesd a halad√°st. Az argumentum lehet≈ëv√© teszi, hogy k√ºl√∂nb√∂z≈ë mintav√©telez≈ëk k√∂z√∂tt v√°lassz, alap√©rtelmezetten `ddim`-en van, √∫gyhogy jobb, ha megv√°ltoztatod!

A `--sample_every_n_epochs` haszn√°lhatod helyette, ami els≈ëbbs√©get √©lvez a l√©p√©sekkel szemben. A `k_` el≈ëtag karras-t jelent, √©s az `_a` ut√≥tag ancestral-t.

```py
    --sample_prompts=/training_dir/sample-prompts.txt --sample_sampler="euler_a" --sample_every_n_steps=100
```

A Pony eset√©ben az aj√°nl√°som a rajzfilmszer≈± k√©pekhez az `euler_a`, a realisztikushoz pedig a `k_dpm_2`.

A mintav√©telez√©si lehet≈ës√©geid a k√∂vetkez≈ëk:

```bash
ddim, pndm, lms, euler, euler_a, heun, dpm_2, dpm_2_a, dpmsolver, dpmsolver++, dpmsingle, k_lms, k_euler, k_euler_a, k_dpm_2, k_dpm_2_a
```

---

Teh√°t az eg√©sz √≠gy n√©zne ki:

```python
accelerate launch --num_cpu_threads_per_process=2  "./sdxl_train_network.py" \
    --lowram \
    --pretrained_model_name_or_path="/ponydiffusers/" \
    --train_data_dir="/training_dir" \
    --resolution="1024,1024" \
    --output_dir="/output_dir" \
    --enable_bucket \
    --min_bucket_reso=256 \
    --max_bucket_reso=1024 \
    --network_alpha=4 \
    --save_model_as="safetensors" \
    --network_module="lycoris.kohya" \
    --network_args \
               "preset=full" \
               "conv_dim=256" \
               "conv_alpha=4" \
               "use_tucker=False" \
               "use_scalar=False" \
               "rank_dropout_scale=False" \
               "algo=locon" \
               "train_norm=False" \
               "block_dims=8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8" \
               "block_alphas=0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625,0.0625" \
    --network_dropout=0 \
    --lr_scheduler="cosine" \
    --learning_rate=0.0001 \
    --unet_lr=0.0001 \
    --text_encoder_lr=0.0001 \
    --network_dim=8 \
    --output_name="yifftoolkit" \
    --scale_weight_norms=1 \
    --no_half_vae \
    --save_every_n_epochs=50 \
    --mixed_precision="fp16" \
    --save_precision="fp16" \
    --caption_extension=".txt" \
    --cache_latents \
    --cache_latents_to_disk \
    --optimizer_type="AdamW" \
    --max_grad_norm=1 \
    --keep_tokens=1 \
    --max_data_loader_n_workers=8 \
    --bucket_reso_steps=32 \
    --multires_noise_iterations=10 \
    --multires_noise_discount=0.1 \
    --log_prefix=xl-locon \
    --gradient_accumulation_steps=6 \
    --gradient_checkpointing \
    --train_batch_size=8 \
    --dataset_repeats=0 \
    --max_train_steps=400 \
    --shuffle_caption \
    --sdpa \
    --sample_prompts=/training_dir/sample-prompts.txt \
    --sample_sampler="euler_a" \
    --sample_every_n_steps=100
```

## Zsugor√≠t√°s

---

Most, hogy a tan√≠t√°sod befejez≈ëd√∂tt √©s elk√©sz√ºlt az els≈ë LoRA-d, cs√∂kkents√ºk a m√©ret√©t egy jelent≈ës<abbr title="A LyCORIS sokat zsugorodik ezzel a folyamattal, de ez kev√©sb√© √©szrevehet≈ë a hagyom√°nyos LoRA-kn√°l, de √≠gy is kevesebb zajt fogsz kapni!">\*</abbr> m√©rt√©kben. A cs√∂kkentett f√°jlm√©ret mellett ez seg√≠t a LoRA-dnak jobban m≈±k√∂dni m√°s modellekkel, √©s nagyban seg√≠t olyan helyzetekben, ahol el√©g sok van bel≈ël√ºk egym√°sra halmozva, a kimenetben tapasztalhat√≥ teljesen elhanyagolhat√≥ k√ºl√∂nbs√©g√©rt cser√©be, amit √©n nem nevezn√©k _min≈ës√©gnek_, a megfelel≈ë be√°ll√≠t√°sokkal.

Ehhez a folyamathoz a [resize_lora](https://github.com/elias-gaeros/resize_lora)-t fogjuk haszn√°lni.

```bash
git clone https://github.com/elias-gaeros/resize_lora
cd resize_lora
```

Gy≈ëz≈ëdj meg r√≥la, hogy a `torch`, `tqdm`, `safetensors` telep√≠tve van a Python k√∂rnyezetedben. Ezut√°n futtasd a k√∂vetkez≈ë parancsot:

```bash
python resize_lora.py -o {output_directory} -r fro_ckpt=1,thr=-3.55 model.safetensors lora.safetensors
```

Csak cser√©ld ki az `{output_directory}`-t a k√≠v√°nt kimeneti k√∂nyvt√°raddal √©s a `model.safetensors`-t azzal a checkpointtal, amit a LoRA betan√≠t√°s√°hoz haszn√°lt√°l, vagy amivel az √∫j LoRA-dat haszn√°lni szeretn√©d, √©s a `lora.safetensors`-t azzal a LoRA-val, amit le szeretn√©l zsugor√≠tani.

Nyugodtan k√≠s√©rletezz b√°rmelyik SVD recepttel, amelyekr≈ël a projekt README-j√©ben olvashatsz, az √©n aj√°nl√°som nyilv√°nval√≥an csak szem√©lyes elfogults√°g, de pr√≥b√°ltam [tesztelni](https://huggingface.co/k4d3/yiff_toolkit/resolve/main/static/shrunk/by_beksinski-shrink-plot/beksinski-shrunk-plot.png?download=true), [sokat](https://huggingface.co/k4d3/yiff_toolkit/tree/main/static/shrunk), hogy m√°soknak ne kelljen!

## L√©p√©sek vs Epoch√°k

---

Egy modell tan√≠t√°sakor fontos meg√©rteni a k√ºl√∂nbs√©get a l√©p√©sek √©s az epoch√°k k√∂z√∂tt. Mindkett≈ë kulcsfontoss√°g√∫ koncepci√≥ a tan√≠t√°si folyamatban, de k√ºl√∂nb√∂z≈ë c√©lokat szolg√°lnak.

### L√©p√©sek

Egy l√©p√©s a tan√≠t√°si folyamat egyetlen iter√°ci√≥j√°t jelenti, ahol a modell feldolgoz egy adagot az adatokb√≥l, √©s friss√≠ti a param√©tereit az abb√≥l az adagb√≥l sz√°m√≠tott vesztes√©g alapj√°n. A l√©p√©sek sz√°m√°t √°ltal√°ban az adagm√©ret √©s a teljes tan√≠t√°si adatmennyis√©g hat√°rozza meg. M√°s sz√≥val, egy l√©p√©s a modell param√©tereinek egyetlen friss√≠t√©se.

### Epoch√°k

Egy epocha ezzel szemben a teljes tan√≠t√°si adatk√©szleten val√≥ egy teljes √°thalad√°st jelent. Egy epocha egyen√©rt√©k≈± az eg√©sz adatk√©szlet egyszeri feldolgoz√°s√°val, ahol minden adag egy l√©p√©sek sorozat√°ban ker√ºl feldolgoz√°sra. Az epoch√°k sz√°ma hat√°rozza meg, hogy h√°nyszor l√°tja a modell a teljes tan√≠t√°si adatk√©szletet a tan√≠t√°s sor√°n.

Az illusztr√°l√°shoz vegy√ºnk egy p√©ld√°t, ahol a tan√≠t√°si adatk√©szlet 1000 k√©pb≈ël √°ll, az adagm√©ret 10, √©s √∂sszesen 10 epocha van. Ebben az esetben:

- A modell 100 l√©p√©st fog feldolgozni epochonk√©nt (1000 k√©p / 10 k√©p adagonk√©nt).
- A modell 10-szer fogja l√°tni az eg√©sz adatk√©szletet, ahol minden epocha 100 l√©p√©sb≈ël √°ll.

A l√©p√©sek √©s epoch√°k k√∂z√∂tti k√ºl√∂nbs√©g meg√©rt√©se kulcsfontoss√°g√∫ a tan√≠t√°si param√©terek, p√©ld√°ul a tanul√°si r√°ta √ºtemez√©s be√°ll√≠t√°s√°hoz, √©s a modell halad√°s√°nak nyomon k√∂vet√©s√©hez a tan√≠t√°s sor√°n.

### Gradiens Akkumul√°ci√≥

A gradiens akkumul√°ci√≥ egy olyan technika, amely cs√∂kkenti a m√©ly neur√°lis h√°l√≥zatok tan√≠t√°s√°nak mem√≥riaig√©ny√©t. √ögy m≈±k√∂dik, hogy t√∂bb iter√°ci√≥n kereszt√ºl gy≈±jti a vesztes√©gf√ºggv√©ny gradienseit a modell param√©tereire vonatkoz√≥an, ahelyett, hogy minden iter√°ci√≥n√°l kisz√°m√≠tan√° a gradienseket. Ez nagyobb adagm√©retet √©s hat√©konyabb GPU mem√≥riahaszn√°latot tesz lehet≈ëv√©.

A LoRA tan√≠t√°s kontextus√°ban a gradiens akkumul√°ci√≥ haszn√°lhat√≥ a tan√≠t√°si folyamat stabilit√°s√°nak √©s hat√©konys√°g√°nak jav√≠t√°s√°ra. A gradiensek t√∂bb iter√°ci√≥n kereszt√ºli akkumul√°l√°s√°val a modell hat√©konyabban tanulhatja meg felismerni a mint√°kat az adatokban, ami jobb teljes√≠tm√©nyhez vezethet.

A gradiens akkumul√°ci√≥ haszn√°lat√°hoz a LoRA tan√≠t√°sban a k√∂vetkez≈ë argumentumot adhatod a tan√≠t√°si parancsodhoz:

```bash
--gradient_accumulation_steps=6
```

Fontos megjegyezni, hogy az epochonk√©nti l√©p√©sek sz√°m√°t az adagm√©ret √©s a teljes tan√≠t√°si adatmennyis√©g hat√°rozza meg. Ez√©rt gradiens akkumul√°ci√≥ haszn√°latakor az epochonk√©nti l√©p√©sek sz√°ma a teljes tan√≠t√°si adatk√©szlet feldolgoz√°s√°hoz sz√ºks√©ges iter√°ci√≥k sz√°ma lesz, nem pedig az adagok sz√°ma. Ez a k√ºl√∂nbs√©g fontos a tanul√°si r√°ta √ºtemez√©s be√°ll√≠t√°sakor √©s a modell halad√°s√°nak nyomon k√∂vet√©sekor a tan√≠t√°s sor√°n.

## V√°ltoz√°sok Nyomon K√∂vet√©se

---

Szeretem az `--output_name`-nek egy relev√°ns nevet adni, hogy biztosan tudjam, pontosan mit v√°ltoztattam an√©lk√ºl, hogy √°t kellene n√©znem a metaadatokat.

{{< blurhash
src="/images/sd-scripts/keep_track_of_changes.png"
blurhash="L8SigQ00?b~qxtofs;j]tMoesroN"
width="522"
height="261"
alt="A k√©p egy sz√°m√≠t√≥g√©pes k√≥dfel√ºlet k√©perny≈ëk√©p√©t mutatja k√ºl√∂nb√∂z≈ë param√©terekkel √©s be√°ll√≠t√°sokkal kiemelve. A h√°tt√©r feh√©r, a sz√∂veg z√∂ld √©s lila sz√≠n≈±. A kulcsfontoss√°g√∫ param√©terek k√∂z√∂tt szerepel a 'network_dropout' √©s az 'lr', amelyek egy g√©pi tanul√°si modell tan√≠t√°si folyamat√°nak be√°ll√≠t√°sait jelzik. A k√∂z√©ps≈ë r√©szek azt sugallj√°k, hogy a kimeneti n√©v fel√ºlvizsg√°lat alatt √°ll. Ez a k√©p relev√°ns azok sz√°m√°ra, akik neur√°lis h√°l√≥zatok tan√≠t√°s√°t konfigur√°lj√°k."
>}}

## Tensorboard

---

A Tensorboard-ot enged√©lyezheted a k√∂vetkez≈ëk konfigur√°ci√≥dhoz ad√°s√°val:

```bash
    --log_prefix=xl-locon \
    --log_with=tensorboard \
    --logging_dir=/output_dir/logs \
```

You will of course need to [install](https://www.tensorflow.org/install/pip) Tensorboard to actually view your training and after that you just need to use this in your output directory:

```bash
tensorboard --logdir=logs
```

Ezut√°n megnyithatod a b√∂ng√©sz≈ëdben a [http://localhost:6006/](http://localhost:6006/) c√≠men, √©s pr√≥b√°lhatsz teaf√ºvet olvasni, √∂√∂√∂, bocs√°nat! √ögy √©rtettem, vesztes√©gi g√∂rb√©ket!
