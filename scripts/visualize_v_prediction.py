"""
Visualize the v-prediction parameterization of the diffusion process.

This script creates a three-panel visualization showing:

1. LEFT: The noisy image (x_t)
    - Shows how the original image is corrupted with noise over time
    - Starts as the original image at t=0
    - Progressively becomes noisier as t increases
    - Generated by mixing the original image and noise based on the angle φ

2. MIDDLE: The raw latent velocity field (v_φ)
    - Shows the rate of change in latent space
    - Displayed at 1/8 resolution (VAE latent space)
    - Normalized to [0,1] range for visualization
    - Brighter areas indicate stronger rates of change

3. RIGHT: The decoded velocity field
    - The velocity field decoded back to image space
    - Shows what the latent velocities "look like" visually
    - Same resolution as the original image
    - Helps interpret the meaning of the velocity predictions

The visualization includes an overlay showing:
    t: Current timestep
    φ: Angle in radians (controls noise level)
    ᾱ: Cumulative signal scaling factor

Mathematical details:
    z_φ = cos(φ)x + sin(φ)ε      (noisy image)
    v_φ = cos(φ)ε - sin(φ)x      (velocity field)
    where:
        x is the original image/latent
        ε is random Gaussian noise
        φ is derived from the noise schedule: φ = arctan(σ_t/α_t)
        σ_t is the noise scaling at time t
        α_t is the signal scaling at time t

The v-prediction formulation is important because:
1. It provides a more stable parameterization for diffusion models
2. The velocity field directly represents the optimal denoising direction
3. Models can learn to predict v_φ instead of the noise ε
4. The angle φ provides a natural way to control noise levels

Usage:
    python visualize_v_prediction.py --image path/to/image.png [options]

Options:
    --steps: Number of diffusion steps (default: 10)
    --fps: Frames per second in output video (default: 30)
    --vae: VAE model to use (default: stabilityai/sd-vae-ft-mse)
    --1step: Generate only a single frame from the middle timestep

Requirements:
    - torch
    - torchvision
    - PIL
    - numpy
    - diffusers
    - ffmpeg (for video generation)
"""

import numpy as np
from PIL import Image, ImageDraw, ImageFont
import torch
import torchvision.transforms as transforms
from pathlib import Path
import argparse
import os
from diffusers import AutoencoderKL

def load_and_preprocess_image(image_path):
    """Load and preprocess an image to tensor, maintaining original resolution."""
    image = Image.open(image_path).convert('RGB')
    # Ensure dimensions are multiples of 8 (VAE requirement) while keeping aspect ratio
    width, height = image.size
    new_width = (width // 8) * 8
    new_height = (height // 8) * 8
    if new_width != width or new_height != height:
        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    
    transform = transforms.Compose([
        transforms.ToTensor(),
    ])
    return transform(image).unsqueeze(0)  # Add batch dimension

def encode_to_latents(vae, image):
    """
    Encode image to latent space using VAE.
    
    Handles both older and newer VAE model versions by checking for different API patterns:
    - Older versions return a LatentDistribution object with latent_dist
    - Newer versions return the distribution directly
    
    The latents are scaled by the appropriate scaling factor:
    - Uses vae.config.scaling_factor if available
    - Falls back to 0.18215 (standard SD scaling) if not
    
    Args:
        vae: The VAE model to use for encoding
        image: Input image tensor in [0,1] range, shape [B,C,H,W]
        
    Returns:
        Scaled latent tensor with shape [B,C,H/8,W/8]
    """
    with torch.no_grad():
        # Updated to handle newer VAE API
        latent_dist = vae.encode(image)
        if hasattr(latent_dist, 'latent_dist'):
            latents = latent_dist.latent_dist.sample()
            latents = latents * vae.config.scaling_factor
        else:
            # For newer versions that return the distribution directly
            latents = latent_dist.sample()
            latents = latents * 0.18215  # Standard SD scaling factor
    return latents

def decode_from_latents(vae, latents):
    """Decode latents back to image space using VAE."""
    with torch.no_grad():
        # Handle both older and newer VAE versions
        if hasattr(vae.config, 'scaling_factor'):
            latents = latents / vae.config.scaling_factor
        else:
            latents = latents / 0.18215
        image = vae.decode(latents).sample
    return image

def v_prediction_step(z_0, t, betas):
    """
    Compute one step of the v-prediction diffusion process.
    
    For a given timestep t, this function:
    1. Calculates the noise level (σ_t) and signal scaling (α_t)
    2. Computes the angle φ = arctan(σ_t/α_t)
    3. Generates random noise ε
    4. Produces:
       - Noisy latent: z_φ = cos(φ)z_0 + sin(φ)ε
       - Velocity field: v_φ = cos(φ)ε - sin(φ)z_0
    
    Args:
        z_0: Original latent tensor
        t: Current timestep
        betas: Complete noise schedule
        
    Returns:
        z_phi: Noisy version of the latent
        v_phi: Velocity field showing rate of change
        phi_t: Current angle in radians
    """
    # Calculate alphas (signal scaling) and sigma (noise level)
    alphas = 1 - betas
    alphas_cumprod = torch.cumprod(alphas, dim=0)
    alpha_t = torch.sqrt(alphas_cumprod[t])  # Signal scaling factor
    sigma_t = torch.sqrt(1 - alphas_cumprod[t])  # Noise scaling factor
    
    # Calculate phi (angle) from alpha and sigma
    # φ = arctan(σ_t/α_t) represents progress through diffusion
    phi_t = torch.arctan2(sigma_t, alpha_t)
    
    # Generate noise in latent space
    epsilon = torch.randn_like(z_0)
    
    # Calculate noisy latent z_phi using angular parameterization
    # z_φ = cos(φ)z + sin(φ)ε
    # - At φ = 0: z_φ ≈ z (original latent)
    # - At φ = π/4: z_φ = equal mix of latent and noise
    # - At φ = π/2: z_φ ≈ ε (pure noise)
    z_phi = torch.cos(phi_t) * z_0 + torch.sin(phi_t) * epsilon
    
    # Calculate velocity field v_phi (direction of change in latent space)
    # v_φ = d(z_φ)/d(φ) = cos(φ)ε - sin(φ)z
    # This shows the instantaneous rate of change at each point in latent space
    v_phi = torch.cos(phi_t) * epsilon - torch.sin(phi_t) * z_0
    
    return z_phi, v_phi, phi_t

def create_v_prediction_animation(z_phis, v_phis, decoded_v_phis, phis, betas, output_path, fps=30):
    """
    Create an MP4 animation showing the three views of the v-prediction process.
    
    Creates a canvas with:
    - Left: Full resolution noisy image progression
    - Middle: Raw latent velocity field (1/8 resolution)
    - Right: Decoded velocity field (full resolution)
    
    Adds text overlay showing:
    - Current timestep t
    - Angle φ in radians
    - Cumulative scaling factor ᾱ
    
    Args:
        z_phis: List of decoded noisy images
        v_phis: List of raw velocity fields
        decoded_v_phis: List of decoded velocity fields
        phis: List of angles
        betas: Noise schedule
        output_path: Where to save the MP4
        fps: Frames per second
    """
    # Convert first tensor to numpy to get dimensions
    first_frame = z_phis[0].squeeze().permute(1, 2, 0).numpy()
    first_frame = np.clip(first_frame, 0, 1)
    first_frame = (first_frame * 255).astype(np.uint8)
    height, width = first_frame.shape[:2]
    
    # Calculate latent dimensions (1/8 of original)
    latent_height = height // 8
    latent_width = width // 8
    
    # Create a larger canvas to accommodate three visualizations and text
    canvas_width = width * 2 + latent_width + 40  # Add padding between images
    canvas_height = max(height, latent_height) + 60  # Add 60px for text
    
    # Create temporary directory for frames
    temp_dir = Path('temp_frames')
    temp_dir.mkdir(exist_ok=True)
    
    # Calculate alphas for overlay text
    alphas = 1 - betas
    alphas_cumprod = torch.cumprod(alphas, dim=0)
    
    # Process each frame
    for i in range(len(z_phis)):
        # Create canvas
        canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')
        
        # Process z_phi (full resolution image)
        if i < len(z_phis):
            z_frame = z_phis[i].squeeze().permute(1, 2, 0).numpy()
            z_frame = np.clip(z_frame, 0, 1)
            z_frame = (z_frame * 255).astype(np.uint8)
            z_pil = Image.fromarray(z_frame)
            canvas.paste(z_pil, (0, 60))
        
        # Process v_phi (latent resolution)
        if i > 0 and i <= len(v_phis):
            # Raw latent velocity field
            v_frame = v_phis[i-1].squeeze().permute(1, 2, 0).numpy()
            v_frame = (v_frame - v_frame.min()) / (v_frame.max() - v_frame.min())
            v_frame = (v_frame * 255).astype(np.uint8)
            v_pil = Image.fromarray(v_frame)
            canvas.paste(v_pil, (width + 20, 60))
            
            # Decoded velocity field
            decoded_v_frame = decoded_v_phis[i-1].squeeze().permute(1, 2, 0).numpy()
            decoded_v_frame = np.clip(decoded_v_frame, 0, 1)
            decoded_v_frame = (decoded_v_frame * 255).astype(np.uint8)
            decoded_v_pil = Image.fromarray(decoded_v_frame)
            canvas.paste(decoded_v_pil, (width + latent_width + 40, 60))
        
        # Add text
        draw = ImageDraw.Draw(canvas)
        try:
            font = ImageFont.truetype("segoeui.ttf", 32)
        except:
            try:
                font = ImageFont.truetype("DejaVuSans.ttf", 32)
            except:
                try:
                    font = ImageFont.truetype("Arial Unicode.ttf", 32)
                except:
                    font = ImageFont.load_default()
        
        if i == 0:
            text = "Noisy Image | Latent Velocity (1/8) | Decoded Velocity"
        else:
            text = f't = {i-1}    φ = {phis[i-1]:.4f}    ᾱ = {alphas_cumprod[i-1]:.4f}'
        
        # Center text
        text_bbox = draw.textbbox((0, 0), text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_x = (canvas_width - text_width) // 2
        
        # Draw text
        draw.text((text_x, 20), text, fill='black', font=font)
        
        # Save frame
        frame_path = temp_dir / f'frame_{i:04d}.png'
        canvas.save(frame_path, 'PNG')
    
    # Convert frames to video using ffmpeg
    ffmpeg_cmd = (
        f'ffmpeg -y -framerate {fps} -i "{temp_dir}/frame_%04d.png" '
        f'-c:v libx264 -preset veryslow '
        f'-crf 15 '
        f'-x264-params "aq-mode=3:aq-strength=0.8" '
        f'-b:v 30M -maxrate 40M -bufsize 60M '
        f'-pix_fmt yuv420p '
        f'-movflags +faststart '
        f'-color_range 1 -colorspace 1 -color_primaries 1 -color_trc 1 '
        f'"{output_path}"'
    )
    
    ret = os.system(ffmpeg_cmd)
    if ret != 0:
        raise RuntimeError(f"FFmpeg conversion failed with return code {ret}")
    
    # Clean up temporary files
    for frame in temp_dir.glob('*.png'):
        frame.unlink()
    temp_dir.rmdir()

def save_single_frame(z_phi, v_phi, decoded_v_phi, phi, alpha_cumprod, output_path):
    """
    Save a single frame showing all three views of the v-prediction process.
    
    Creates a side-by-side visualization with:
    - Left: Full resolution noisy image
    - Middle: Raw latent velocity field (1/8 resolution)
    - Right: Decoded velocity field (full resolution)
    
    Adds text overlay showing:
    - Current angle φ in radians
    - Cumulative scaling factor ᾱ
    
    Args:
        z_phi: Decoded noisy image tensor
        v_phi: Raw velocity field tensor
        decoded_v_phi: Decoded velocity field tensor
        phi: Current angle in radians
        alpha_cumprod: Cumulative scaling factor
        output_path: Where to save the PNG
    """
    # Convert image to numpy
    z_frame = z_phi.squeeze().permute(1, 2, 0).numpy()
    z_frame = np.clip(z_frame, 0, 1)
    z_frame = (z_frame * 255).astype(np.uint8)
    height, width = z_frame.shape[:2]
    
    # Calculate latent dimensions
    latent_height = height // 8
    latent_width = width // 8
    
    # Create canvas
    canvas_width = width * 2 + latent_width + 40
    canvas_height = max(height, latent_height) + 60
    canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')
    
    # Paste noisy image
    z_pil = Image.fromarray(z_frame)
    canvas.paste(z_pil, (0, 60))
    
    # Process raw latent velocity field
    v_frame = v_phi.squeeze().permute(1, 2, 0).numpy()
    v_frame = (v_frame - v_frame.min()) / (v_frame.max() - v_frame.min())
    v_frame = (v_frame * 255).astype(np.uint8)
    v_pil = Image.fromarray(v_frame)
    canvas.paste(v_pil, (width + 20, 60))
    
    # Process decoded velocity field
    decoded_v_frame = decoded_v_phi.squeeze().permute(1, 2, 0).numpy()
    decoded_v_frame = np.clip(decoded_v_frame, 0, 1)
    decoded_v_frame = (decoded_v_frame * 255).astype(np.uint8)
    decoded_v_pil = Image.fromarray(decoded_v_frame)
    canvas.paste(decoded_v_pil, (width + latent_width + 40, 60))
    
    # Add text
    draw = ImageDraw.Draw(canvas)
    try:
        font = ImageFont.truetype("segoeui.ttf", 32)
    except:
        try:
            font = ImageFont.truetype("DejaVuSans.ttf", 32)
        except:
            try:
                font = ImageFont.truetype("Arial Unicode.ttf", 32)
            except:
                font = ImageFont.load_default()
    
    text = f'φ = {phi:.4f}    ᾱ = {alpha_cumprod:.4f}'
    text_bbox = draw.textbbox((0, 0), text, font=font)
    text_width = text_bbox[2] - text_bbox[0]
    text_x = (canvas_width - text_width) // 2
    draw.text((text_x, 20), text, fill='black', font=font)
    
    # Save frame
    canvas.save(output_path, 'PNG')

def normalize_velocity_field(v_phi, print_stats=False, timestep=None):
    """
    Normalize velocity field to prepare it for VAE decoding.
    
    The normalization process:
    1. Center the data by subtracting mean
    2. Scale to unit standard deviation
    3. Clip to reasonable range to avoid extreme values
    4. Scale to [-1, 1] range (VAE expected input range)
    5. Scale down by 0.5 to reduce saturation
    6. Add 0.5 offset to center around gray
    
    Args:
        v_phi: Raw velocity field tensor
        print_stats: Whether to print normalization statistics
        timestep: Optional timestep for stats printing
        
    Returns:
        Normalized velocity field tensor in [0, 1] range
    """
    if print_stats:
        prefix = f"(t={timestep}) " if timestep is not None else ""
        print(f"\n{prefix}Raw velocity field stats:")
        print(f"Mean: {v_phi.mean().item():.4f}")
        print(f"Std: {v_phi.std().item():.4f}")
        print(f"Min: {v_phi.min().item():.4f}")
        print(f"Max: {v_phi.max().item():.4f}")
    
    # Center the data
    v_phi_centered = v_phi #- v_phi.mean()
    
    # Scale to unit standard deviation
    v_phi_norm = v_phi_centered / v_phi.std()
    
    # Clip to reasonable range (±3 standard deviations)
    v_phi_norm = torch.clamp(v_phi_norm, -3, 3)
    
    # Scale to [-1, 1]
    v_phi_norm = v_phi_norm / 3
    
    # Scale down to avoid saturation
    v_phi_norm = v_phi_norm * 0.5
    
    # Center around gray
    v_phi_norm = v_phi_norm + 0.5
    
    if print_stats:
        print(f"\n{prefix}Normalized velocity field stats (before decoding):")
        print(f"Mean: {v_phi_norm.mean().item():.4f}")
        print(f"Std: {v_phi_norm.std().item():.4f}")
        print(f"Min: {v_phi_norm.min().item():.4f}")
        print(f"Max: {v_phi_norm.max().item():.4f}")
    
    return v_phi_norm

def visualize_v_prediction(image_path, vae_model="stabilityai/sd-vae-ft-mse", num_steps=10, beta_min=1e-4, beta_max=0.02, fps=30, single_frame=False):
    """
    Create a visualization of the v-prediction diffusion process.
    
    This function:
    1. Loads and encodes the input image to latent space
    2. Computes the v-prediction process for specified steps
    3. Creates either:
       - A video showing the full process
       - A single frame from the middle timestep
    
    The visualization shows three views:
    - Noisy image progression
    - Raw latent velocity field
    - Decoded velocity field
    
    Args:
        image_path: Path to input image
        vae_model: HuggingFace model ID for the VAE
        num_steps: Number of diffusion steps
        beta_min: Minimum noise level
        beta_max: Maximum noise level
        fps: Frames per second for video
        single_frame: If True, only generate middle frame
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load VAE model
    vae = AutoencoderKL.from_pretrained(vae_model, torch_dtype=torch.float32).to(device)
    
    # Load and preprocess image at original resolution (adjusted to multiple of 8)
    x_0 = load_and_preprocess_image(image_path).to(device)
    
    # Encode image to latent space
    z_0 = encode_to_latents(vae, x_0)
    
    if single_frame:
        # Use middle timestep for single frame
        t = num_steps // 2
        betas = torch.linspace(beta_min, beta_max, num_steps)
        alphas = 1 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        
        # Get single frame
        z_phi, v_phi, phi_t = v_prediction_step(z_0, t, betas)
        x_t = decode_from_latents(vae, z_phi)
        
        # Normalize and decode velocity field
        v_phi_norm = normalize_velocity_field(v_phi, print_stats=True)
        decoded_v_phi = decode_from_latents(vae, v_phi_norm)
        
        # Create output directory
        output_dir = Path('static/comfyui')
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save single frame
        save_single_frame(x_t.cpu(), v_phi.cpu(), decoded_v_phi.cpu(), 
                         phi_t.item(), alphas_cumprod[t].item(),
                         output_dir / 'v_prediction_frame.png')
        return
    
    betas = torch.linspace(beta_min, beta_max, num_steps)
    
    # Store decoded images and velocities
    z_phis = [x_0.cpu()]
    v_phis = []
    decoded_v_phis = []
    phis = []
    
    # Perform v-prediction steps in latent space
    for t in range(num_steps):
        z_phi, v_phi, phi_t = v_prediction_step(z_0, t, betas)
        x_t = decode_from_latents(vae, z_phi)
        z_phis.append(x_t.cpu())
        v_phis.append(v_phi.cpu())
        
        # Normalize and decode velocity field
        print_stats = (t == num_steps // 2)  # Only print stats for middle frame
        v_phi_norm = normalize_velocity_field(v_phi, print_stats=print_stats, timestep=t)
        decoded_v_phi = decode_from_latents(vae, v_phi_norm)
        decoded_v_phis.append(decoded_v_phi.cpu())
        phis.append(phi_t.item())
    
    # Create output directory
    output_dir = Path('static/comfyui')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Create animation
    create_v_prediction_animation(z_phis, v_phis, decoded_v_phis, phis, betas, 
                                output_dir / 'v_prediction.mp4', fps=fps)

def main():
    parser = argparse.ArgumentParser(description='Visualize the v-prediction process')
    parser.add_argument('--image', type=str, required=True, help='Path to input image')
    parser.add_argument('--vae', type=str, default="stabilityai/sd-vae-ft-mse", help='VAE model to use')
    parser.add_argument('--steps', type=int, default=10, help='Number of diffusion steps')
    parser.add_argument('--fps', type=int, default=30, help='Frames per second for the animation')
    parser.add_argument('--1step', action='store_true', help='Render only one frame from the middle')
    args = parser.parse_args()
    
    visualize_v_prediction(
        args.image,
        vae_model=args.vae,
        num_steps=args.steps,
        fps=args.fps,
        single_frame=getattr(args, '1step', False)
    )

if __name__ == '__main__':
    main() 